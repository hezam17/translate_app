{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7644218,"sourceType":"datasetVersion","datasetId":4455619},{"sourceId":7917364,"sourceType":"datasetVersion","datasetId":4612039}],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install  TTS","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-18T00:02:38.813500Z","iopub.execute_input":"2024-04-18T00:02:38.813876Z","iopub.status.idle":"2024-04-18T00:04:08.902443Z","shell.execute_reply.started":"2024-04-18T00:02:38.813845Z","shell.execute_reply":"2024-04-18T00:04:08.901529Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting TTS\n  Downloading TTS-0.22.0-cp310-cp310-manylinux1_x86_64.whl.metadata (21 kB)\nRequirement already satisfied: cython>=0.29.30 in /opt/conda/lib/python3.10/site-packages (from TTS) (3.0.8)\nRequirement already satisfied: scipy>=1.11.2 in /opt/conda/lib/python3.10/site-packages (from TTS) (1.11.4)\nRequirement already satisfied: torch>=2.1 in /opt/conda/lib/python3.10/site-packages (from TTS) (2.1.2)\nRequirement already satisfied: torchaudio in /opt/conda/lib/python3.10/site-packages (from TTS) (2.1.2)\nRequirement already satisfied: soundfile>=0.12.0 in /opt/conda/lib/python3.10/site-packages (from TTS) (0.12.1)\nRequirement already satisfied: librosa>=0.10.0 in /opt/conda/lib/python3.10/site-packages (from TTS) (0.10.1)\nCollecting scikit-learn>=1.3.0 (from TTS)\n  Downloading scikit_learn-1.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\nCollecting inflect>=5.6.0 (from TTS)\n  Downloading inflect-7.2.0-py3-none-any.whl.metadata (21 kB)\nRequirement already satisfied: tqdm>=4.64.1 in /opt/conda/lib/python3.10/site-packages (from TTS) (4.66.1)\nCollecting anyascii>=0.3.0 (from TTS)\n  Downloading anyascii-0.3.2-py3-none-any.whl.metadata (1.5 kB)\nRequirement already satisfied: pyyaml>=6.0 in /opt/conda/lib/python3.10/site-packages (from TTS) (6.0.1)\nRequirement already satisfied: fsspec>=2023.6.0 in /opt/conda/lib/python3.10/site-packages (from TTS) (2024.2.0)\nRequirement already satisfied: aiohttp>=3.8.1 in /opt/conda/lib/python3.10/site-packages (from TTS) (3.9.1)\nCollecting packaging>=23.1 (from TTS)\n  Downloading packaging-24.0-py3-none-any.whl.metadata (3.2 kB)\nRequirement already satisfied: flask>=2.0.1 in /opt/conda/lib/python3.10/site-packages (from TTS) (3.0.2)\nCollecting pysbd>=0.3.4 (from TTS)\n  Downloading pysbd-0.3.4-py3-none-any.whl.metadata (6.1 kB)\nRequirement already satisfied: umap-learn>=0.5.1 in /opt/conda/lib/python3.10/site-packages (from TTS) (0.5.5)\nCollecting pandas<2.0,>=1.4 (from TTS)\n  Downloading pandas-1.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\nRequirement already satisfied: matplotlib>=3.7.0 in /opt/conda/lib/python3.10/site-packages (from TTS) (3.7.5)\nCollecting trainer>=0.0.32 (from TTS)\n  Downloading trainer-0.0.36-py3-none-any.whl.metadata (8.1 kB)\nCollecting coqpit>=0.0.16 (from TTS)\n  Downloading coqpit-0.0.17-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: jieba in /opt/conda/lib/python3.10/site-packages (from TTS) (0.42.1)\nCollecting pypinyin (from TTS)\n  Downloading pypinyin-0.51.0-py2.py3-none-any.whl.metadata (12 kB)\nCollecting hangul-romanize (from TTS)\n  Downloading hangul_romanize-0.1.0-py3-none-any.whl.metadata (1.2 kB)\nCollecting gruut==2.2.3 (from gruut[de,es,fr]==2.2.3->TTS)\n  Downloading gruut-2.2.3.tar.gz (73 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting jamo (from TTS)\n  Downloading jamo-0.4.1-py3-none-any.whl.metadata (2.3 kB)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from TTS) (3.2.4)\nCollecting g2pkk>=0.1.1 (from TTS)\n  Downloading g2pkk-0.1.2-py3-none-any.whl.metadata (2.0 kB)\nCollecting bangla (from TTS)\n  Downloading bangla-0.0.2-py2.py3-none-any.whl.metadata (4.5 kB)\nCollecting bnnumerizer (from TTS)\n  Downloading bnnumerizer-0.0.2.tar.gz (4.7 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting bnunicodenormalizer (from TTS)\n  Downloading bnunicodenormalizer-0.1.6.tar.gz (39 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting einops>=0.6.0 (from TTS)\n  Downloading einops-0.7.0-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: transformers>=4.33.0 in /opt/conda/lib/python3.10/site-packages (from TTS) (4.38.1)\nCollecting encodec>=0.1.1 (from TTS)\n  Downloading encodec-0.1.1.tar.gz (3.7 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: unidecode>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from TTS) (1.3.8)\nCollecting num2words (from TTS)\n  Downloading num2words-0.5.13-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: spacy>=3 in /opt/conda/lib/python3.10/site-packages (from spacy[ja]>=3->TTS) (3.7.2)\nCollecting numpy==1.22.0 (from TTS)\n  Downloading numpy-1.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.0 kB)\nRequirement already satisfied: numba>=0.57.0 in /opt/conda/lib/python3.10/site-packages (from TTS) (0.58.1)\nRequirement already satisfied: Babel<3.0.0,>=2.8.0 in /opt/conda/lib/python3.10/site-packages (from gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS) (2.14.0)\nCollecting dateparser~=1.1.0 (from gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS)\n  Downloading dateparser-1.1.8-py2.py3-none-any.whl.metadata (27 kB)\nCollecting gruut-ipa<1.0,>=0.12.0 (from gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS)\n  Downloading gruut-ipa-0.13.0.tar.gz (101 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting gruut_lang_en~=2.0.0 (from gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS)\n  Downloading gruut_lang_en-2.0.0.tar.gz (15.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.2/15.2 MB\u001b[0m \u001b[31m77.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting jsonlines~=1.2.0 (from gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS)\n  Downloading jsonlines-1.2.0-py2.py3-none-any.whl.metadata (1.3 kB)\nCollecting networkx<3.0.0,>=2.5.0 (from gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS)\n  Downloading networkx-2.8.8-py3-none-any.whl.metadata (5.1 kB)\nCollecting python-crfsuite~=0.9.7 (from gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS)\n  Downloading python_crfsuite-0.9.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\nCollecting gruut_lang_de~=2.0.0 (from gruut[de,es,fr]==2.2.3->TTS)\n  Downloading gruut_lang_de-2.0.0.tar.gz (18.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m74.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting gruut_lang_es~=2.0.0 (from gruut[de,es,fr]==2.2.3->TTS)\n  Downloading gruut_lang_es-2.0.0.tar.gz (31.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m54.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting gruut_lang_fr~=2.0.0 (from gruut[de,es,fr]==2.2.3->TTS)\n  Downloading gruut_lang_fr-2.0.2.tar.gz (10.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m85.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp>=3.8.1->TTS) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp>=3.8.1->TTS) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp>=3.8.1->TTS) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp>=3.8.1->TTS) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp>=3.8.1->TTS) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp>=3.8.1->TTS) (4.0.3)\nRequirement already satisfied: Werkzeug>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from flask>=2.0.1->TTS) (3.0.1)\nRequirement already satisfied: Jinja2>=3.1.2 in /opt/conda/lib/python3.10/site-packages (from flask>=2.0.1->TTS) (3.1.2)\nRequirement already satisfied: itsdangerous>=2.1.2 in /opt/conda/lib/python3.10/site-packages (from flask>=2.0.1->TTS) (2.1.2)\nRequirement already satisfied: click>=8.1.3 in /opt/conda/lib/python3.10/site-packages (from flask>=2.0.1->TTS) (8.1.7)\nRequirement already satisfied: blinker>=1.6.2 in /opt/conda/lib/python3.10/site-packages (from flask>=2.0.1->TTS) (1.7.0)\nRequirement already satisfied: more-itertools in /opt/conda/lib/python3.10/site-packages (from inflect>=5.6.0->TTS) (10.2.0)\nRequirement already satisfied: typeguard>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from inflect>=5.6.0->TTS) (4.1.5)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from inflect>=5.6.0->TTS) (4.9.0)\nRequirement already satisfied: audioread>=2.1.9 in /opt/conda/lib/python3.10/site-packages (from librosa>=0.10.0->TTS) (3.0.1)\nINFO: pip is looking at multiple versions of librosa to determine which version is compatible with other requirements. This could take a while.\nCollecting librosa>=0.10.0 (from TTS)\n  Downloading librosa-0.10.0.post2-py3-none-any.whl.metadata (8.3 kB)\n  Downloading librosa-0.10.0.post1-py3-none-any.whl.metadata (8.3 kB)\n  Downloading librosa-0.10.0-py3-none-any.whl.metadata (8.3 kB)\nRequirement already satisfied: joblib>=0.14 in /opt/conda/lib/python3.10/site-packages (from librosa>=0.10.0->TTS) (1.3.2)\nRequirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.10/site-packages (from librosa>=0.10.0->TTS) (5.1.1)\nRequirement already satisfied: pooch>=1.0 in /opt/conda/lib/python3.10/site-packages (from librosa>=0.10.0->TTS) (1.8.1)\nRequirement already satisfied: soxr>=0.3.2 in /opt/conda/lib/python3.10/site-packages (from librosa>=0.10.0->TTS) (0.3.7)\nRequirement already satisfied: lazy-loader>=0.1 in /opt/conda/lib/python3.10/site-packages (from librosa>=0.10.0->TTS) (0.3)\nRequirement already satisfied: msgpack>=1.0 in /opt/conda/lib/python3.10/site-packages (from librosa>=0.10.0->TTS) (1.0.7)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->TTS) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->TTS) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->TTS) (4.47.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->TTS) (1.4.5)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->TTS) (9.5.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->TTS) (3.1.1)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->TTS) (2.8.2)\nRequirement already satisfied: docopt>=0.6.2 in /opt/conda/lib/python3.10/site-packages (from num2words->TTS) (0.6.2)\nRequirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /opt/conda/lib/python3.10/site-packages (from numba>=0.57.0->TTS) (0.41.1)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas<2.0,>=1.4->TTS) (2023.3.post1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=1.3.0->TTS) (3.2.0)\nRequirement already satisfied: cffi>=1.0 in /opt/conda/lib/python3.10/site-packages (from soundfile>=0.12.0->TTS) (1.16.0)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/conda/lib/python3.10/site-packages (from spacy>=3->spacy[ja]>=3->TTS) (3.0.12)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from spacy>=3->spacy[ja]>=3->TTS) (1.0.5)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.10/site-packages (from spacy>=3->spacy[ja]>=3->TTS) (1.0.10)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy>=3->spacy[ja]>=3->TTS) (2.0.8)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy>=3->spacy[ja]>=3->TTS) (3.0.9)\nRequirement already satisfied: thinc<8.3.0,>=8.1.8 in /opt/conda/lib/python3.10/site-packages (from spacy>=3->spacy[ja]>=3->TTS) (8.2.2)\nRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/conda/lib/python3.10/site-packages (from spacy>=3->spacy[ja]>=3->TTS) (1.1.2)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.10/site-packages (from spacy>=3->spacy[ja]>=3->TTS) (2.4.8)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.10/site-packages (from spacy>=3->spacy[ja]>=3->TTS) (2.0.10)\nRequirement already satisfied: weasel<0.4.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from spacy>=3->spacy[ja]>=3->TTS) (0.3.4)\nRequirement already satisfied: typer<0.10.0,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from spacy>=3->spacy[ja]>=3->TTS) (0.9.0)\nRequirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/conda/lib/python3.10/site-packages (from spacy>=3->spacy[ja]>=3->TTS) (6.4.0)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from spacy>=3->spacy[ja]>=3->TTS) (2.31.0)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/conda/lib/python3.10/site-packages (from spacy>=3->spacy[ja]>=3->TTS) (2.5.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from spacy>=3->spacy[ja]>=3->TTS) (69.0.3)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.10/site-packages (from spacy>=3->spacy[ja]>=3->TTS) (3.3.0)\nCollecting sudachipy!=0.6.1,>=0.5.2 (from spacy[ja]>=3->TTS)\n  Downloading SudachiPy-0.6.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\nCollecting sudachidict-core>=20211220 (from spacy[ja]>=3->TTS)\n  Downloading SudachiDict_core-20240409-py3-none-any.whl.metadata (2.5 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=2.1->TTS) (3.13.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=2.1->TTS) (1.12)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from trainer>=0.0.32->TTS) (5.9.3)\nRequirement already satisfied: tensorboard in /opt/conda/lib/python3.10/site-packages (from trainer>=0.0.32->TTS) (2.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.33.0->TTS) (0.20.3)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.33.0->TTS) (2023.12.25)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.33.0->TTS) (0.15.2)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.33.0->TTS) (0.4.2)\nRequirement already satisfied: pynndescent>=0.5 in /opt/conda/lib/python3.10/site-packages (from umap-learn>=0.5.1->TTS) (0.5.11)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from nltk->TTS) (1.16.0)\nRequirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.0->soundfile>=0.12.0->TTS) (2.21)\nRequirement already satisfied: tzlocal in /opt/conda/lib/python3.10/site-packages (from dateparser~=1.1.0->gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS) (5.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from Jinja2>=3.1.2->flask>=2.0.1->TTS) (2.1.3)\nRequirement already satisfied: platformdirs>=2.5.0 in /opt/conda/lib/python3.10/site-packages (from pooch>=1.0->librosa>=0.10.0->TTS) (4.2.0)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3->spacy[ja]>=3->TTS) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3->spacy[ja]>=3->TTS) (2.14.6)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy>=3->spacy[ja]>=3->TTS) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy>=3->spacy[ja]>=3->TTS) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy>=3->spacy[ja]>=3->TTS) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy>=3->spacy[ja]>=3->TTS) (2024.2.2)\nRequirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/conda/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy>=3->spacy[ja]>=3->TTS) (0.7.10)\nRequirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy>=3->spacy[ja]>=3->TTS) (0.1.4)\nRequirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from weasel<0.4.0,>=0.1.0->spacy>=3->spacy[ja]>=3->TTS) (0.16.0)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=2.1->TTS) (1.3.0)\nRequirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.10/site-packages (from tensorboard->trainer>=0.0.32->TTS) (1.4.0)\nRequirement already satisfied: grpcio>=1.48.2 in /opt/conda/lib/python3.10/site-packages (from tensorboard->trainer>=0.0.32->TTS) (1.51.1)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard->trainer>=0.0.32->TTS) (2.26.1)\nRequirement already satisfied: google-auth-oauthlib<2,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard->trainer>=0.0.32->TTS) (1.2.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard->trainer>=0.0.32->TTS) (3.5.2)\nRequirement already satisfied: protobuf<4.24,>=3.19.6 in /opt/conda/lib/python3.10/site-packages (from tensorboard->trainer>=0.0.32->TTS) (3.20.3)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard->trainer>=0.0.32->TTS) (0.7.2)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard->trainer>=0.0.32->TTS) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard->trainer>=0.0.32->TTS) (0.3.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard->trainer>=0.0.32->TTS) (4.9)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard->trainer>=0.0.32->TTS) (1.3.1)\nRequirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->trainer>=0.0.32->TTS) (0.5.1)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard->trainer>=0.0.32->TTS) (3.2.2)\nDownloading TTS-0.22.0-cp310-cp310-manylinux1_x86_64.whl (938 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m938.0/938.0 kB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading numpy-1.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m72.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading coqpit-0.0.17-py3-none-any.whl (13 kB)\nDownloading einops-0.7.0-py3-none-any.whl (44 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading g2pkk-0.1.2-py3-none-any.whl (25 kB)\nDownloading inflect-7.2.0-py3-none-any.whl (34 kB)\nDownloading librosa-0.10.0-py3-none-any.whl (252 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m252.9/252.9 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading num2words-0.5.13-py3-none-any.whl (143 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.3/143.3 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading packaging-24.0-py3-none-any.whl (53 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.5/53.5 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pandas-1.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m81.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading pysbd-0.3.4-py3-none-any.whl (71 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading scikit_learn-1.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m87.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading trainer-0.0.36-py3-none-any.whl (51 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.2/51.2 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading bangla-0.0.2-py2.py3-none-any.whl (6.2 kB)\nDownloading hangul_romanize-0.1.0-py3-none-any.whl (4.6 kB)\nDownloading jamo-0.4.1-py3-none-any.whl (9.5 kB)\nDownloading pypinyin-0.51.0-py2.py3-none-any.whl (1.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m56.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading dateparser-1.1.8-py2.py3-none-any.whl (293 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.8/293.8 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading jsonlines-1.2.0-py2.py3-none-any.whl (7.6 kB)\nDownloading networkx-2.8.8-py3-none-any.whl (2.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading python_crfsuite-0.9.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading SudachiDict_core-20240409-py3-none-any.whl (72.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading SudachiPy-0.6.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m69.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: gruut, encodec, bnnumerizer, bnunicodenormalizer, gruut-ipa, gruut_lang_de, gruut_lang_en, gruut_lang_es, gruut_lang_fr\n  Building wheel for gruut (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for gruut: filename=gruut-2.2.3-py3-none-any.whl size=75792 sha256=578d161f4ba89585e626d128afd2e6677ee36d5786405fc695ba993f5647531c\n  Stored in directory: /root/.cache/pip/wheels/fc/57/a8/f9de532daf5214f53644f20f3a9e6f69269453c87df9c0a817\n  Building wheel for encodec (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for encodec: filename=encodec-0.1.1-py3-none-any.whl size=45759 sha256=ac819b5c1faf900273c169bce35126e014546676eeda62824b9328718ab5a997\n  Stored in directory: /root/.cache/pip/wheels/fc/36/cb/81af8b985a5f5e0815312d5e52b41263237af07b977e6bcbf3\n  Building wheel for bnnumerizer (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for bnnumerizer: filename=bnnumerizer-0.0.2-py3-none-any.whl size=5259 sha256=8b73955b29fd26b5e3034f368856dec321941f9e21858192181a696d18080a80\n  Stored in directory: /root/.cache/pip/wheels/59/6b/e8/223172e7d5c9f72df3ea1a0d9258f3a8ab5b28e827728edef5\n  Building wheel for bnunicodenormalizer (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for bnunicodenormalizer: filename=bnunicodenormalizer-0.1.6-py3-none-any.whl size=22779 sha256=9213a0c90c81374a5b3a6727c291d03e4d02547a0f726218f55c65585dc82111\n  Stored in directory: /root/.cache/pip/wheels/f4/d7/e9/16732a619cbf5a63fdc9f6e2f9eb5fcf73fa023735237330e9\n  Building wheel for gruut-ipa (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for gruut-ipa: filename=gruut_ipa-0.13.0-py3-none-any.whl size=104873 sha256=4db1d983df79c1d194ded409b112f7969787d82a6a5bb3bfee0093df68b657a3\n  Stored in directory: /root/.cache/pip/wheels/7b/18/49/e4f500ecdf0babe757953f844e4d7cd1ea81c5503c09bfe984\n  Building wheel for gruut_lang_de (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for gruut_lang_de: filename=gruut_lang_de-2.0.0-py3-none-any.whl size=18498182 sha256=d463a10914a609290f9d5db8d1560b4c9d57f9d261b19e6f8932a42bbe101e1d\n  Stored in directory: /root/.cache/pip/wheels/95/9a/05/cfce98f0c41a1a540f15708c4a02df190b82d84cf91ef6bc7f\n  Building wheel for gruut_lang_en (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for gruut_lang_en: filename=gruut_lang_en-2.0.0-py3-none-any.whl size=15297179 sha256=6b8564e85ed2e9e3c8537d0bc22dd8fd73b39874d7cd227537242f60741d09d5\n  Stored in directory: /root/.cache/pip/wheels/10/9c/fb/77c655a9fbd78cdb9935d0ab65d80ddd0a3bcf7dbe18261650\n  Building wheel for gruut_lang_es (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for gruut_lang_es: filename=gruut_lang_es-2.0.0-py3-none-any.whl size=32173797 sha256=0e96e6b608b6b87dbaa373f743d2ad03bdf277ae61af94e1202fbb8a6553f9ca\n  Stored in directory: /root/.cache/pip/wheels/9b/0a/90/788d92c07744b329b9283e37b29b064f5db6b1bb0442a1a19b\n  Building wheel for gruut_lang_fr (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for gruut_lang_fr: filename=gruut_lang_fr-2.0.2-py3-none-any.whl size=10968767 sha256=e584df00585bfeec6e5bfe0849417e8e0780400be137a302653b30b05688b3bb\n  Stored in directory: /root/.cache/pip/wheels/db/21/be/d0436e3f1cf9bf38b9bb9b4a476399c77a1ab19f7172b45e19\nSuccessfully built gruut encodec bnnumerizer bnunicodenormalizer gruut-ipa gruut_lang_de gruut_lang_en gruut_lang_es gruut_lang_fr\nInstalling collected packages: sudachipy, python-crfsuite, jamo, hangul-romanize, gruut_lang_fr, gruut_lang_es, gruut_lang_en, gruut_lang_de, bnunicodenormalizer, bnnumerizer, bangla, sudachidict-core, pysbd, pypinyin, packaging, numpy, num2words, networkx, jsonlines, gruut-ipa, einops, coqpit, anyascii, pandas, inflect, g2pkk, dateparser, scikit-learn, gruut, librosa, encodec, trainer, TTS\n  Attempting uninstall: packaging\n    Found existing installation: packaging 21.3\n    Uninstalling packaging-21.3:\n      Successfully uninstalled packaging-21.3\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.26.4\n    Uninstalling numpy-1.26.4:\n      Successfully uninstalled numpy-1.26.4\n  Attempting uninstall: networkx\n    Found existing installation: networkx 3.2.1\n    Uninstalling networkx-3.2.1:\n      Successfully uninstalled networkx-3.2.1\n  Attempting uninstall: pandas\n    Found existing installation: pandas 2.1.4\n    Uninstalling pandas-2.1.4:\n      Successfully uninstalled pandas-2.1.4\n  Attempting uninstall: scikit-learn\n    Found existing installation: scikit-learn 1.2.2\n    Uninstalling scikit-learn-1.2.2:\n      Successfully uninstalled scikit-learn-1.2.2\n  Attempting uninstall: librosa\n    Found existing installation: librosa 0.10.1\n    Uninstalling librosa-0.10.1:\n      Successfully uninstalled librosa-0.10.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cubinlinker, which is not installed.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 23.8.0 requires ptxcompiler, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\nkeras-cv 0.8.2 requires keras-core, which is not installed.\nkeras-nlp 0.8.1 requires keras-core, which is not installed.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\nalbumentations 1.4.0 requires numpy>=1.24.4, but you have numpy 1.22.0 which is incompatible.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 11.0.0 which is incompatible.\nbeatrix-jupyterlab 2023.128.151533 requires jupyterlab~=3.6.0, but you have jupyterlab 4.1.2 which is incompatible.\nchex 0.1.85 requires numpy>=1.24.1, but you have numpy 1.22.0 which is incompatible.\ncudf 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.3.0 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\ndask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\ndipy 1.8.0 requires numpy>=1.22.4, but you have numpy 1.22.0 which is incompatible.\ndistributed 2023.7.1 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\ngoogle-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 24.0 which is incompatible.\ninequality 1.0.1 requires numpy>=1.23, but you have numpy 1.22.0 which is incompatible.\njupyterlab 4.1.2 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.0.3 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmapclassify 2.6.1 requires numpy>=1.23, but you have numpy 1.22.0 which is incompatible.\nmizani 0.11.0 requires numpy>=1.23.0, but you have numpy 1.22.0 which is incompatible.\nmizani 0.11.0 requires pandas>=2.1.0, but you have pandas 1.5.3 which is incompatible.\nmomepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nosmnx 1.9.1 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\nplotnine 0.13.0 requires numpy>=1.23.0, but you have numpy 1.22.0 which is incompatible.\nplotnine 0.13.0 requires pandas>=2.1.0, but you have pandas 1.5.3 which is incompatible.\npyldavis 3.4.1 requires numpy>=1.24.2, but you have numpy 1.22.0 which is incompatible.\npyldavis 3.4.1 requires pandas>=2.0.0, but you have pandas 1.5.3 which is incompatible.\npylibraft 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.3.0 which is incompatible.\npywavelets 1.5.0 requires numpy<2.0,>=1.22.4, but you have numpy 1.22.0 which is incompatible.\nraft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\nrmm 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.3.0 which is incompatible.\nspglm 1.1.0 requires numpy>=1.23, but you have numpy 1.22.0 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nspreg 1.4.2 requires numpy>=1.23, but you have numpy 1.22.0 which is incompatible.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.0.5 which is incompatible.\ntensorflow 2.15.0 requires numpy<2.0.0,>=1.23.5, but you have numpy 1.22.0 which is incompatible.\ntensorstore 0.1.53 requires ml-dtypes>=0.3.1, but you have ml-dtypes 0.2.0 which is incompatible.\nwoodwork 0.28.0 requires numpy<2.0.0,>=1.25.0, but you have numpy 1.22.0 which is incompatible.\nxarray 2024.2.0 requires numpy>=1.23, but you have numpy 1.22.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed TTS-0.22.0 anyascii-0.3.2 bangla-0.0.2 bnnumerizer-0.0.2 bnunicodenormalizer-0.1.6 coqpit-0.0.17 dateparser-1.1.8 einops-0.7.0 encodec-0.1.1 g2pkk-0.1.2 gruut-2.2.3 gruut-ipa-0.13.0 gruut_lang_de-2.0.0 gruut_lang_en-2.0.0 gruut_lang_es-2.0.0 gruut_lang_fr-2.0.2 hangul-romanize-0.1.0 inflect-7.2.0 jamo-0.4.1 jsonlines-1.2.0 librosa-0.10.0 networkx-2.8.8 num2words-0.5.13 numpy-1.22.0 packaging-24.0 pandas-1.5.3 pypinyin-0.51.0 pysbd-0.3.4 python-crfsuite-0.9.10 scikit-learn-1.4.2 sudachidict-core-20240409 sudachipy-0.6.8 trainer-0.0.36\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install  faster_whisper==0.9.0 gradio==4.7.1\n!pip install -q typing_extensions==4.8 numpy==1.26.2","metadata":{"execution":{"iopub.status.busy":"2024-04-18T00:04:08.904457Z","iopub.execute_input":"2024-04-18T00:04:08.904861Z","iopub.status.idle":"2024-04-18T00:04:59.029082Z","shell.execute_reply.started":"2024-04-18T00:04:08.904826Z","shell.execute_reply":"2024-04-18T00:04:59.027863Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting faster_whisper==0.9.0\n  Downloading faster_whisper-0.9.0-py3-none-any.whl.metadata (11 kB)\nCollecting gradio==4.7.1\n  Downloading gradio-4.7.1-py3-none-any.whl.metadata (17 kB)\nCollecting av==10.* (from faster_whisper==0.9.0)\n  Downloading av-10.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.5 kB)\nCollecting ctranslate2<4,>=3.17 (from faster_whisper==0.9.0)\n  Downloading ctranslate2-3.24.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\nRequirement already satisfied: huggingface-hub>=0.13 in /opt/conda/lib/python3.10/site-packages (from faster_whisper==0.9.0) (0.20.3)\nCollecting tokenizers<0.15,>=0.13 (from faster_whisper==0.9.0)\n  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nCollecting onnxruntime<2,>=1.14 (from faster_whisper==0.9.0)\n  Downloading onnxruntime-1.17.3-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.4 kB)\nRequirement already satisfied: aiofiles<24.0,>=22.0 in /opt/conda/lib/python3.10/site-packages (from gradio==4.7.1) (22.1.0)\nRequirement already satisfied: altair<6.0,>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from gradio==4.7.1) (5.2.0)\nRequirement already satisfied: fastapi in /opt/conda/lib/python3.10/site-packages (from gradio==4.7.1) (0.108.0)\nCollecting ffmpy (from gradio==4.7.1)\n  Downloading ffmpy-0.3.2.tar.gz (5.5 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting gradio-client==0.7.0 (from gradio==4.7.1)\n  Downloading gradio_client-0.7.0-py3-none-any.whl.metadata (7.1 kB)\nRequirement already satisfied: httpx in /opt/conda/lib/python3.10/site-packages (from gradio==4.7.1) (0.27.0)\nRequirement already satisfied: importlib-resources<7.0,>=1.3 in /opt/conda/lib/python3.10/site-packages (from gradio==4.7.1) (6.1.1)\nRequirement already satisfied: jinja2<4.0 in /opt/conda/lib/python3.10/site-packages (from gradio==4.7.1) (3.1.2)\nRequirement already satisfied: markupsafe~=2.0 in /opt/conda/lib/python3.10/site-packages (from gradio==4.7.1) (2.1.3)\nRequirement already satisfied: matplotlib~=3.0 in /opt/conda/lib/python3.10/site-packages (from gradio==4.7.1) (3.7.5)\nRequirement already satisfied: numpy~=1.0 in /opt/conda/lib/python3.10/site-packages (from gradio==4.7.1) (1.22.0)\nRequirement already satisfied: orjson~=3.0 in /opt/conda/lib/python3.10/site-packages (from gradio==4.7.1) (3.9.10)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from gradio==4.7.1) (24.0)\nRequirement already satisfied: pandas<3.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from gradio==4.7.1) (1.5.3)\nRequirement already satisfied: pillow<11.0,>=8.0 in /opt/conda/lib/python3.10/site-packages (from gradio==4.7.1) (9.5.0)\nRequirement already satisfied: pydantic>=2.0 in /opt/conda/lib/python3.10/site-packages (from gradio==4.7.1) (2.5.3)\nRequirement already satisfied: pydub in /opt/conda/lib/python3.10/site-packages (from gradio==4.7.1) (0.25.1)\nCollecting python-multipart (from gradio==4.7.1)\n  Downloading python_multipart-0.0.9-py3-none-any.whl.metadata (2.5 kB)\nRequirement already satisfied: pyyaml<7.0,>=5.0 in /opt/conda/lib/python3.10/site-packages (from gradio==4.7.1) (6.0.1)\nRequirement already satisfied: requests~=2.0 in /opt/conda/lib/python3.10/site-packages (from gradio==4.7.1) (2.31.0)\nCollecting semantic-version~=2.0 (from gradio==4.7.1)\n  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\nCollecting tomlkit==0.12.0 (from gradio==4.7.1)\n  Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\nRequirement already satisfied: typer<1.0,>=0.9 in /opt/conda/lib/python3.10/site-packages (from typer[all]<1.0,>=0.9->gradio==4.7.1) (0.9.0)\nRequirement already satisfied: typing-extensions~=4.0 in /opt/conda/lib/python3.10/site-packages (from gradio==4.7.1) (4.9.0)\nRequirement already satisfied: uvicorn>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from gradio==4.7.1) (0.25.0)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from gradio-client==0.7.0->gradio==4.7.1) (2024.2.0)\nCollecting websockets<12.0,>=10.0 (from gradio-client==0.7.0->gradio==4.7.1)\n  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\nRequirement already satisfied: jsonschema>=3.0 in /opt/conda/lib/python3.10/site-packages (from altair<6.0,>=4.2.0->gradio==4.7.1) (4.20.0)\nRequirement already satisfied: toolz in /opt/conda/lib/python3.10/site-packages (from altair<6.0,>=4.2.0->gradio==4.7.1) (0.12.1)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from ctranslate2<4,>=3.17->faster_whisper==0.9.0) (69.0.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.13->faster_whisper==0.9.0) (3.13.1)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.13->faster_whisper==0.9.0) (4.66.1)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio==4.7.1) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio==4.7.1) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio==4.7.1) (4.47.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio==4.7.1) (1.4.5)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio==4.7.1) (3.1.1)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio==4.7.1) (2.8.2)\nCollecting coloredlogs (from onnxruntime<2,>=1.14->faster_whisper==0.9.0)\n  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: flatbuffers in /opt/conda/lib/python3.10/site-packages (from onnxruntime<2,>=1.14->faster_whisper==0.9.0) (23.5.26)\nRequirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from onnxruntime<2,>=1.14->faster_whisper==0.9.0) (3.20.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from onnxruntime<2,>=1.14->faster_whisper==0.9.0) (1.12)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio==4.7.1) (2023.3.post1)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic>=2.0->gradio==4.7.1) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic>=2.0->gradio==4.7.1) (2.14.6)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests~=2.0->gradio==4.7.1) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests~=2.0->gradio==4.7.1) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests~=2.0->gradio==4.7.1) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests~=2.0->gradio==4.7.1) (2024.2.2)\nCollecting huggingface-hub>=0.13 (from faster_whisper==0.9.0)\n  Downloading huggingface_hub-0.17.3-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.10/site-packages (from typer<1.0,>=0.9->typer[all]<1.0,>=0.9->gradio==4.7.1) (8.1.7)\nRequirement already satisfied: colorama<0.5.0,>=0.4.3 in /opt/conda/lib/python3.10/site-packages (from typer[all]<1.0,>=0.9->gradio==4.7.1) (0.4.6)\nRequirement already satisfied: shellingham<2.0.0,>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from typer[all]<1.0,>=0.9->gradio==4.7.1) (1.5.4)\nRequirement already satisfied: rich<14.0.0,>=10.11.0 in /opt/conda/lib/python3.10/site-packages (from typer[all]<1.0,>=0.9->gradio==4.7.1) (13.7.0)\nRequirement already satisfied: h11>=0.8 in /opt/conda/lib/python3.10/site-packages (from uvicorn>=0.14.0->gradio==4.7.1) (0.14.0)\nRequirement already satisfied: starlette<0.33.0,>=0.29.0 in /opt/conda/lib/python3.10/site-packages (from fastapi->gradio==4.7.1) (0.32.0.post1)\nRequirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from httpx->gradio==4.7.1) (4.2.0)\nRequirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx->gradio==4.7.1) (1.0.4)\nRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx->gradio==4.7.1) (1.3.0)\nRequirement already satisfied: attrs>=22.2.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==4.7.1) (23.2.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==4.7.1) (2023.12.1)\nRequirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==4.7.1) (0.32.1)\nRequirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==4.7.1) (0.16.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio==4.7.1) (1.16.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio==4.7.1) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio==4.7.1) (2.17.2)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio->httpx->gradio==4.7.1) (1.2.0)\nCollecting humanfriendly>=9.1 (from coloredlogs->onnxruntime<2,>=1.14->faster_whisper==0.9.0)\n  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->onnxruntime<2,>=1.14->faster_whisper==0.9.0) (1.3.0)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio==4.7.1) (0.1.2)\nDownloading faster_whisper-0.9.0-py3-none-any.whl (1.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading gradio-4.7.1-py3-none-any.whl (16.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m66.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading av-10.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.0/31.0 MB\u001b[0m \u001b[31m55.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading gradio_client-0.7.0-py3-none-any.whl (302 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.7/302.7 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\nDownloading ctranslate2-3.24.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.8/36.8 MB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading onnxruntime-1.17.3-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m72.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\nDownloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\nDownloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m925.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m262.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m541.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: ffmpy\n  Building wheel for ffmpy (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for ffmpy: filename=ffmpy-0.3.2-py3-none-any.whl size=5584 sha256=e92cc3118ecb15d43003f54c8447f44af295be92a0190e70ddb83972b05b33fc\n  Stored in directory: /root/.cache/pip/wheels/bd/65/9a/671fc6dcde07d4418df0c592f8df512b26d7a0029c2a23dd81\nSuccessfully built ffmpy\nInstalling collected packages: ffmpy, av, websockets, tomlkit, semantic-version, python-multipart, humanfriendly, ctranslate2, huggingface-hub, coloredlogs, tokenizers, onnxruntime, gradio-client, faster_whisper, gradio\n  Attempting uninstall: websockets\n    Found existing installation: websockets 12.0\n    Uninstalling websockets-12.0:\n      Successfully uninstalled websockets-12.0\n  Attempting uninstall: tomlkit\n    Found existing installation: tomlkit 0.12.3\n    Uninstalling tomlkit-0.12.3:\n      Successfully uninstalled tomlkit-0.12.3\n  Attempting uninstall: huggingface-hub\n    Found existing installation: huggingface-hub 0.20.3\n    Uninstalling huggingface-hub-0.20.3:\n      Successfully uninstalled huggingface-hub-0.20.3\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.15.2\n    Uninstalling tokenizers-0.15.2:\n      Successfully uninstalled tokenizers-0.15.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntransformers 4.38.1 requires huggingface-hub<1.0,>=0.19.3, but you have huggingface-hub 0.17.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed av-10.0.0 coloredlogs-15.0.1 ctranslate2-3.24.0 faster_whisper-0.9.0 ffmpy-0.3.2 gradio-4.7.1 gradio-client-0.7.0 huggingface-hub-0.17.3 humanfriendly-10.0 onnxruntime-1.17.3 python-multipart-0.0.9 semantic-version-2.10.0 tokenizers-0.14.1 tomlkit-0.12.0 websockets-11.0.3\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cubinlinker, which is not installed.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 23.8.0 requires ptxcompiler, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\nkeras-nlp 0.8.1 requires keras-core, which is not installed.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\ntts 0.22.0 requires numpy==1.22.0; python_version <= \"3.10\", but you have numpy 1.26.2 which is incompatible.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.2 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 11.0.0 which is incompatible.\ncudf 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.3.0 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\ndask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\njupyterlab 4.1.2 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.0.3 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmizani 0.11.0 requires pandas>=2.1.0, but you have pandas 1.5.3 which is incompatible.\nmomepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nosmnx 1.9.1 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\nplotnine 0.13.0 requires pandas>=2.1.0, but you have pandas 1.5.3 which is incompatible.\npyldavis 3.4.1 requires pandas>=2.0.0, but you have pandas 1.5.3 which is incompatible.\npylibraft 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.3.0 which is incompatible.\nraft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\nrmm 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.3.0 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.0.5 which is incompatible.\ntensorstore 0.1.53 requires ml-dtypes>=0.3.1, but you have ml-dtypes 0.2.0 which is incompatible.\ntransformers 4.38.1 requires huggingface-hub<1.0,>=0.19.3, but you have huggingface-hub 0.17.3 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"!pip install --upgrade huggingface_hub","metadata":{"execution":{"iopub.status.busy":"2024-04-18T00:04:59.030449Z","iopub.execute_input":"2024-04-18T00:04:59.030754Z","iopub.status.idle":"2024-04-18T00:05:12.760314Z","shell.execute_reply.started":"2024-04-18T00:04:59.030727Z","shell.execute_reply":"2024-04-18T00:05:12.759384Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Requirement already satisfied: huggingface_hub in /opt/conda/lib/python3.10/site-packages (0.17.3)\nCollecting huggingface_hub\n  Downloading huggingface_hub-0.22.2-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (2024.2.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (24.0)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (6.0.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (2.31.0)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (4.66.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (4.8.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (2024.2.2)\nDownloading huggingface_hub-0.22.2-py3-none-any.whl (388 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.9/388.9 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: huggingface_hub\n  Attempting uninstall: huggingface_hub\n    Found existing installation: huggingface-hub 0.17.3\n    Uninstalling huggingface-hub-0.17.3:\n      Successfully uninstalled huggingface-hub-0.17.3\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntokenizers 0.14.1 requires huggingface_hub<0.18,>=0.16.4, but you have huggingface-hub 0.22.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed huggingface_hub-0.22.2\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport gc\nimport torchaudio\nimport pandas\nfrom faster_whisper import WhisperModel\nfrom glob import glob\n\nfrom tqdm import tqdm\n\nimport torch\nimport torchaudio\n# torch.set_num_threads(1)\n\nfrom TTS.tts.layers.xtts.tokenizer import multilingual_cleaners\n\ntorch.set_num_threads(16)\n\n\nimport os\n\naudio_types = (\".wav\", \".mp3\", \".flac\")\n\n\ndef list_audios(basePath, contains=None):\n    # return the set of files that are valid\n    return list_files(basePath, validExts=audio_types, contains=contains)\n\ndef list_files(basePath, validExts=None, contains=None):\n    audios_path = []\n    # loop over the directory structure\n    for (rootDir, dirNames, filenames) in os.walk(basePath):\n        # loop over the filenames in the current directory\n        for filename in filenames:\n            # if the contains string is not none and the filename does not contain\n            # the supplied string, then ignore the file\n            if contains is not None and filename.find(contains) == -1:\n                continue\n\n            # determine the file extension of the current file\n            ext = filename[filename.rfind(\".\"):].lower()\n\n            # check to see if the file is an audio and should be processed\n            if validExts is None or ext.endswith(validExts):\n                # construct the path to the audio and yield it\n                audioPath = os.path.join(rootDir, filename)\n                audios_path.append(audioPath)\n    return audios_path\n\ndef format_audio_list(audio_files, target_language=\"en\", out_path=None, buffer=0.2, eval_percentage=0.15, speaker_name=\"coqui\", gradio_progress=None):\n    audio_total_size = 0\n    # make sure that ooutput file exists\n    os.makedirs(out_path, exist_ok=True)\n\n    # Loading Whisper\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\" \n\n    print(\"Loading Whisper Model!\")\n    asr_model = WhisperModel(\"large-v2\", device=device, compute_type=\"float32\")\n\n    metadata = {\"audio_file\": [], \"text\": [], \"speaker_name\": []}\n    \n    \n    if gradio_progress is not None:\n        tqdm_object = gradio_progress.tqdm(audio_files, desc=\"Formatting...\")\n    else:\n        tqdm_object = tqdm(list_audios(audio_files))\n    for audio_path in tqdm_object:\n        wav, sr = torchaudio.load(audio_path)\n        # stereo to mono if needed\n        if wav.size(0) != 1:\n            wav = torch.mean(wav, dim=0, keepdim=True)\n\n        wav = wav.squeeze()\n        audio_total_size += (wav.size(-1) / sr)\n\n        segments, _ = asr_model.transcribe(audio_path, word_timestamps=True, language=target_language)\n        segments = list(segments)\n        i = 0\n        sentence = \"\"\n        sentence_start = None\n        first_word = True\n        # added all segments words in a unique list\n        words_list = []\n        for _, segment in enumerate(segments):\n            words = list(segment.words)\n            words_list.extend(words)\n\n        # process each word\n        for word_idx, word in enumerate(words_list):\n            if first_word:\n                sentence_start = word.start\n                # If it is the first sentence, add buffer or get the begining of the file\n                if word_idx == 0:\n                    sentence_start = max(sentence_start - buffer, 0)  # Add buffer to the sentence start\n                else:\n                    # get previous sentence end\n                    previous_word_end = words_list[word_idx - 1].end\n                    # add buffer or get the silence midle between the previous sentence and the current one\n                    sentence_start = max(sentence_start - buffer, (previous_word_end + sentence_start)/2)\n\n                sentence = word.word\n                first_word = False\n            else:\n                sentence += word.word\n\n            if word.word[-1] in [\"!\", \".\", \"?\"]:\n                sentence = sentence[1:]\n                # Expand number and abbreviations plus normalization\n                sentence = multilingual_cleaners(sentence, target_language)\n                audio_file_name, _ = os.path.splitext(os.path.basename(audio_path))\n\n                audio_file = f\"wavs/{audio_file_name}_{str(i).zfill(8)}.wav\"\n\n                # Check for the next word's existence\n                if word_idx + 1 < len(words_list):\n                    next_word_start = words_list[word_idx + 1].start\n                else:\n                    # If don't have more words it means that it is the last sentence then use the audio len as next word start\n                    next_word_start = (wav.shape[0] - 1) / sr\n\n                # Average the current word end and next word start\n                word_end = min((word.end + next_word_start) / 2, word.end + buffer)\n                \n                absoulte_path = os.path.join(out_path, audio_file)\n                os.makedirs(os.path.dirname(absoulte_path), exist_ok=True)\n                i += 1\n                first_word = True\n\n                audio = wav[int(sr*sentence_start):int(sr*word_end)].unsqueeze(0)\n                # if the audio is too short ignore it (i.e < 0.33 seconds)\n                if audio.size(-1) >= sr/3:\n                    torchaudio.save(absoulte_path,\n                        audio,\n                        sr\n                    )\n                else:\n                    continue\n\n                metadata[\"audio_file\"].append(audio_file)\n                metadata[\"text\"].append(sentence)\n                metadata[\"speaker_name\"].append(speaker_name)\n    \n    df = pandas.DataFrame(metadata)\n    df = df.sample(frac=1)\n    num_val_samples = int(len(df)*eval_percentage)\n\n    df_eval = df[:num_val_samples]\n    df_train = df[num_val_samples:]\n\n    df_train = df_train.sort_values('audio_file')\n    train_metadata_path = os.path.join(out_path, \"metadata_train.csv\")\n    df_train.to_csv(train_metadata_path, sep=\"|\", index=False)\n\n    eval_metadata_path = os.path.join(out_path, \"metadata_eval.csv\")\n    df_eval = df_eval.sort_values('audio_file')\n    df_eval.to_csv(eval_metadata_path, sep=\"|\", index=False)\n\n    # deallocate VRAM and RAM\n    del asr_model, df_train, df_eval, df, metadata\n    gc.collect()\n\n    return train_metadata_path, eval_metadata_path, audio_total_size","metadata":{"execution":{"iopub.status.busy":"2024-04-18T00:05:12.762811Z","iopub.execute_input":"2024-04-18T00:05:12.763137Z","iopub.status.idle":"2024-04-18T00:05:32.706777Z","shell.execute_reply.started":"2024-04-18T00:05:12.763109Z","shell.execute_reply":"2024-04-18T00:05:32.705982Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import os\nimport gc\n\nfrom trainer import Trainer, TrainerArgs\n\nfrom TTS.config.shared_configs import BaseDatasetConfig\nfrom TTS.tts.datasets import load_tts_samples\nfrom TTS.tts.layers.xtts.trainer.gpt_trainer import GPTArgs, GPTTrainer, GPTTrainerConfig, XttsAudioConfig\nfrom TTS.utils.manage import ModelManager\n\n\ndef train_gpt(language, num_epochs, batch_size, grad_acumm, train_csv, eval_csv, output_path, max_audio_length=255995):\n    #  Logging parameters\n    RUN_NAME = \"GPT_XTTS_FT\"\n    PROJECT_NAME = \"XTTS_trainer\"\n    DASHBOARD_LOGGER = \"tensorboard\"\n    LOGGER_URI = None\n\n    # Set here the path that the checkpoints will be saved. Default: ./run/training/\n    OUT_PATH = os.path.join(output_path, \"run\", \"training\")\n\n    # Training Parameters\n    OPTIMIZER_WD_ONLY_ON_WEIGHTS = True  # for multi-gpu training please make it False\n    START_WITH_EVAL = False  # if True it will star with evaluation\n    BATCH_SIZE = batch_size  # set here the batch size\n    GRAD_ACUMM_STEPS = grad_acumm  # set here the grad accumulation steps\n\n\n    # Define here the dataset that you want to use for the fine-tuning on.\n    config_dataset = BaseDatasetConfig(\n        formatter=\"coqui\",\n        dataset_name=\"ft_dataset\",\n        path=os.path.dirname(train_csv),\n        meta_file_train=train_csv,\n        meta_file_val=eval_csv,\n        language=language,\n    )\n\n    # Add here the configs of the datasets\n    DATASETS_CONFIG_LIST = [config_dataset]\n\n    # Define the path where XTTS v2.0.1 files will be downloaded\n    CHECKPOINTS_OUT_PATH = os.path.join(OUT_PATH, \"XTTS_v2.0_original_model_files/\")\n    os.makedirs(CHECKPOINTS_OUT_PATH, exist_ok=True)\n\n\n    # DVAE files\n    DVAE_CHECKPOINT_LINK = \"https://coqui.gateway.scarf.sh/hf-coqui/XTTS-v2/main/dvae.pth\"\n    MEL_NORM_LINK = \"https://coqui.gateway.scarf.sh/hf-coqui/XTTS-v2/main/mel_stats.pth\"\n\n    # Set the path to the downloaded files\n    DVAE_CHECKPOINT = os.path.join(CHECKPOINTS_OUT_PATH, os.path.basename(DVAE_CHECKPOINT_LINK))\n    MEL_NORM_FILE = os.path.join(CHECKPOINTS_OUT_PATH, os.path.basename(MEL_NORM_LINK))\n\n    # download DVAE files if needed\n    if not os.path.isfile(DVAE_CHECKPOINT) or not os.path.isfile(MEL_NORM_FILE):\n        print(\" > Downloading DVAE files!\")\n        ModelManager._download_model_files([MEL_NORM_LINK, DVAE_CHECKPOINT_LINK], CHECKPOINTS_OUT_PATH, progress_bar=True)\n\n\n    # Download XTTS v2.0 checkpoint if needed\n    TOKENIZER_FILE_LINK = \"https://coqui.gateway.scarf.sh/hf-coqui/XTTS-v2/main/vocab.json\"\n    XTTS_CHECKPOINT_LINK = \"https://coqui.gateway.scarf.sh/hf-coqui/XTTS-v2/main/model.pth\"\n    XTTS_CONFIG_LINK = \"https://coqui.gateway.scarf.sh/hf-coqui/XTTS-v2/main/config.json\"\n\n    # XTTS transfer learning parameters: You we need to provide the paths of XTTS model checkpoint that you want to do the fine tuning.\n    TOKENIZER_FILE = os.path.join(CHECKPOINTS_OUT_PATH, os.path.basename(TOKENIZER_FILE_LINK))  # vocab.json file\n    XTTS_CHECKPOINT = os.path.join(CHECKPOINTS_OUT_PATH, os.path.basename(XTTS_CHECKPOINT_LINK))  # model.pth file\n    XTTS_CONFIG_FILE = os.path.join(CHECKPOINTS_OUT_PATH, os.path.basename(XTTS_CONFIG_LINK))  # config.json file\n\n    # download XTTS v2.0 files if needed\n    if not os.path.isfile(TOKENIZER_FILE) or not os.path.isfile(XTTS_CHECKPOINT):\n        print(\" > Downloading XTTS v2.0 files!\")\n        ModelManager._download_model_files(\n            [TOKENIZER_FILE_LINK, XTTS_CHECKPOINT_LINK, XTTS_CONFIG_LINK], CHECKPOINTS_OUT_PATH, progress_bar=True\n        )\n\n    # init args and config\n    model_args = GPTArgs(\n        max_conditioning_length=132300,  # 6 secs\n        min_conditioning_length=66150,  # 3 secs\n        debug_loading_failures=False,\n        max_wav_length=max_audio_length,  # ~11.6 seconds\n        max_text_length=200,\n        mel_norm_file=MEL_NORM_FILE,\n        dvae_checkpoint=DVAE_CHECKPOINT,\n        xtts_checkpoint=XTTS_CHECKPOINT,  # checkpoint path of the model that you want to fine-tune\n        tokenizer_file=TOKENIZER_FILE,\n        gpt_num_audio_tokens=1026,\n        gpt_start_audio_token=1024,\n        gpt_stop_audio_token=1025,\n        gpt_use_masking_gt_prompt_approach=True,\n        gpt_use_perceiver_resampler=True,\n    )\n    # define audio config\n    audio_config = XttsAudioConfig(sample_rate=22050, dvae_sample_rate=22050, output_sample_rate=24000)\n    # training parameters config\n    config = GPTTrainerConfig(\n        epochs=num_epochs,\n        output_path=OUT_PATH,\n        model_args=model_args,\n        run_name=RUN_NAME,\n        project_name=PROJECT_NAME,\n        run_description=\"\"\"\n            GPT XTTS training\n            \"\"\",\n        dashboard_logger=DASHBOARD_LOGGER,\n        logger_uri=LOGGER_URI,\n        audio=audio_config,\n        batch_size=BATCH_SIZE,\n        batch_group_size=48,\n        eval_batch_size=BATCH_SIZE,\n        num_loader_workers=8,\n        eval_split_max_size=256,\n        print_step=50,\n        plot_step=100,\n        log_model_step=100,\n        save_step=1000,\n        save_n_checkpoints=1,\n        save_checkpoints=True,\n        # target_loss=\"loss\",\n        print_eval=False,\n        # Optimizer values like tortoise, pytorch implementation with modifications to not apply WD to non-weight parameters.\n        optimizer=\"AdamW\",\n        optimizer_wd_only_on_weights=OPTIMIZER_WD_ONLY_ON_WEIGHTS,\n        optimizer_params={\"betas\": [0.9, 0.96], \"eps\": 1e-8, \"weight_decay\": 1e-2},\n        lr=5e-06,  # learning rate\n        lr_scheduler=\"MultiStepLR\",\n        # it was adjusted accordly for the new step scheme\n        lr_scheduler_params={\"milestones\": [50000 * 18, 150000 * 18, 300000 * 18], \"gamma\": 0.5, \"last_epoch\": -1},\n        test_sentences=[],\n    )\n\n    # init the model from config\n    model = GPTTrainer.init_from_config(config)\n\n    # load training samples\n    train_samples, eval_samples = load_tts_samples(\n        DATASETS_CONFIG_LIST,\n        eval_split=True,\n        eval_split_max_size=config.eval_split_max_size,\n        eval_split_size=config.eval_split_size,\n    )\n\n    # init the trainer and 🚀\n    trainer = Trainer(\n        TrainerArgs(\n            restore_path=None,  # xtts checkpoint is restored via xtts_checkpoint key so no need of restore it using Trainer restore_path parameter\n            skip_train_epoch=False,\n            start_with_eval=START_WITH_EVAL,\n            grad_accum_steps=GRAD_ACUMM_STEPS,\n        ),\n        config,\n        output_path=OUT_PATH,\n        model=model,\n        train_samples=train_samples,\n        eval_samples=eval_samples,\n    )\n    trainer.fit()\n\n    # get the longest text audio file to use as speaker reference\n    samples_len = [len(item[\"text\"].split(\" \")) for item in train_samples]\n    longest_text_idx =  samples_len.index(max(samples_len))\n    speaker_ref = train_samples[longest_text_idx][\"audio_file\"]\n\n    trainer_out_path = trainer.output_path\n\n    # deallocate VRAM and RAM\n    del model, trainer, train_samples, eval_samples\n    gc.collect()\n\n    return XTTS_CONFIG_FILE, XTTS_CHECKPOINT, TOKENIZER_FILE, trainer_out_path, speaker_ref","metadata":{"execution":{"iopub.status.busy":"2024-04-18T00:05:32.716479Z","iopub.execute_input":"2024-04-18T00:05:32.718742Z","iopub.status.idle":"2024-04-18T00:05:34.197429Z","shell.execute_reply.started":"2024-04-18T00:05:32.718708Z","shell.execute_reply":"2024-04-18T00:05:34.196656Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import argparse\nimport os\nimport sys\nimport tempfile\n\nimport gradio as gr\nimport librosa.display\nimport numpy as np\n\nimport os\nimport torch\nimport torchaudio\nimport traceback\n\nfrom TTS.tts.configs.xtts_config import XttsConfig\nfrom TTS.tts.models.xtts import Xtts\n\n\nout_path = \"/kaggle/working/\"\naudio_files = \"/kaggle/input/ttsnewdata/wavs\"\nlang = \"en\"\nXTTS_MODEL = None\n\n\ndef clear_gpu_cache():\n    # clear the GPU cache\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n\n\n\ndef load_model(xtts_checkpoint, xtts_config, xtts_vocab):\n    global XTTS_MODEL\n    clear_gpu_cache()\n    if not xtts_checkpoint or not xtts_config or not xtts_vocab:\n        return \"You need to run the previous steps or manually set the `XTTS checkpoint path`, `XTTS config path`, and `XTTS vocab path` fields !!\"\n    config = XttsConfig()\n    config.load_json(xtts_config)\n    XTTS_MODEL = Xtts.init_from_config(config)\n    print(\"Loading XTTS model! \")\n    XTTS_MODEL.load_checkpoint(config, checkpoint_path=xtts_checkpoint, vocab_path=xtts_vocab, use_deepspeed=False)\n    if torch.cuda.is_available():\n        XTTS_MODEL.cuda()\n\n    print(\"Model Loaded!\")\n    return \"Model Loaded!\"\n\ndef run_tts(lang, tts_text, speaker_audio_file):\n    if XTTS_MODEL is None or not speaker_audio_file:\n        return \"You need to run the previous step to load the model !!\", None, None\n\n    gpt_cond_latent, speaker_embedding = XTTS_MODEL.get_conditioning_latents(audio_path=speaker_audio_file, gpt_cond_len=XTTS_MODEL.config.gpt_cond_len, max_ref_length=XTTS_MODEL.config.max_ref_len, sound_norm_refs=XTTS_MODEL.config.sound_norm_refs)\n    out = XTTS_MODEL.inference(\n        text=tts_text,\n        language=lang,\n        gpt_cond_latent=gpt_cond_latent,\n        speaker_embedding=speaker_embedding,\n        temperature=XTTS_MODEL.config.temperature, # Add custom parameters here\n        length_penalty=XTTS_MODEL.config.length_penalty,\n        repetition_penalty=XTTS_MODEL.config.repetition_penalty,\n        top_k=XTTS_MODEL.config.top_k,\n        top_p=XTTS_MODEL.config.top_p,\n    )\n\n    with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as fp:\n        out[\"wav\"] = torch.tensor(out[\"wav\"]).unsqueeze(0)\n        out_path = fp.name\n        torchaudio.save(out_path, out[\"wav\"], 24000)\n\n    return \"Speech generated !\", out_path, speaker_audio_file\n\n\n\n\n\ndef preprocess_dataset(audio_path, language, out_path, progress=None):\n    clear_gpu_cache()\n    out_path = os.path.join(out_path, \"dataset\")\n    os.makedirs(out_path, exist_ok=True)\n    if audio_path is None:\n        return \"You should provide one or multiple audio files! If you provided it, probably the upload of the files is not finished yet!\", \"\", \"\"\n    else:\n        try:\n            train_meta, eval_meta, audio_total_size = format_audio_list(audio_path, target_language=language, out_path=out_path, gradio_progress=progress)\n        except:\n            traceback.print_exc()\n            error = traceback.format_exc()\n            return f\"The data processing was interrupted due an error !! Please check the console to verify the full error message! \\n Error summary: {error}\", \"\", \"\"\n\n    clear_gpu_cache()\n\n    # if audio total len is less than 2 minutes raise an error\n    if audio_total_size < 120:\n        message = \"The sum of the duration of the audios that you provided should be at least 2 minutes!\"\n        print(message)\n        return message, \"\", \"\"\n\n    print(\"Dataset Processed!\")\n    return \"Dataset Processed!\", train_meta, eval_meta","metadata":{"execution":{"iopub.status.busy":"2024-04-18T00:05:34.198626Z","iopub.execute_input":"2024-04-18T00:05:34.199189Z","iopub.status.idle":"2024-04-18T00:05:37.243356Z","shell.execute_reply.started":"2024-04-18T00:05:34.199163Z","shell.execute_reply":"2024-04-18T00:05:37.242564Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"\ndef train_model(language, train_csv, eval_csv, num_epochs, batch_size, grad_acumm, output_path, max_audio_length):\n    clear_gpu_cache()\n    if not train_csv or not eval_csv:\n        return \"You need to run the data processing step or manually set `Train CSV` and `Eval CSV` fields !\", \"\", \"\", \"\", \"\"\n    try:\n        # convert seconds to waveform frames\n        max_audio_length = int(max_audio_length * 22050)\n        config_path, original_xtts_checkpoint, vocab_file, exp_path, speaker_wav = train_gpt(language, num_epochs, batch_size, grad_acumm, train_csv, eval_csv, output_path=output_path, max_audio_length=max_audio_length)\n    except:\n        traceback.print_exc()\n        error = traceback.format_exc()\n        return f\"The training was interrupted due an error !! Please check the console to check the full error message! \\n Error summary: {error}\", \"\", \"\", \"\", \"\"\n\n    # copy original files to avoid parameters changes issues\n    os.system(f\"cp {config_path} {exp_path}\")\n    os.system(f\"cp {vocab_file} {exp_path}\")\n\n    ft_xtts_checkpoint = os.path.join(exp_path, \"best_model.pth\")\n    print(\"Model training done!\")\n    clear_gpu_cache()\n    return \"Model training done!\", config_path, vocab_file, ft_xtts_checkpoint, speaker_wav\n","metadata":{"execution":{"iopub.status.busy":"2024-04-18T00:05:37.244410Z","iopub.execute_input":"2024-04-18T00:05:37.244686Z","iopub.status.idle":"2024-04-18T00:05:37.252342Z","shell.execute_reply.started":"2024-04-18T00:05:37.244663Z","shell.execute_reply":"2024-04-18T00:05:37.251300Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"tts_language = \"en\" #choices=[\"en\",\"es\",\"fr\",\"de\",\"it\",\"pt\",\"pl\",\"tr\",\"ru\",\"nl\",\"cs\",\"ar\",\"zh\",\"hu\",\"ko\",\"ja\",]\ntts_text = \"This model sounds really good and above all, it's reasonably fast.\"\nnum_epochs = 6\nbatch_size = 2\ngrad_acumm = 1\nmax_audio_length = 11\n\n\nprogress_data, train_csv, eval_csv = preprocess_dataset(audio_files,lang,out_path)\n\nprogress_train, xtts_config, xtts_vocab, xtts_checkpoint, speaker_reference_audio = train_model(lang,\"/kaggle/working/dataset/metadata_train.csv\",\n                                                                                                \"/kaggle/working/dataset/metadata_eval.csv\",\n                                                                                                num_epochs,\n                                                                                                batch_size,\n                                                                                                grad_acumm,\n                                                                                                out_path,\n                                                                                                max_audio_length)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-18T00:05:37.253862Z","iopub.execute_input":"2024-04-18T00:05:37.254448Z","iopub.status.idle":"2024-04-18T00:14:47.493817Z","shell.execute_reply.started":"2024-04-18T00:05:37.254417Z","shell.execute_reply":"2024-04-18T00:14:47.492696Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Loading Whisper Model!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.bin:   0%|          | 0.00/3.09G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e31c40c0e59847f5b126de98271c819c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.20M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"386b257998824e30932f1d2ed442831c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocabulary.txt:   0%|          | 0.00/460k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"593b633aeb62465c94ca9052b9a2772f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/2.80k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"77ce035d06d244118e82f60a7b3d0785"}},"metadata":{}},{"name":"stderr","text":"100%|██████████| 113/113 [03:17<00:00,  1.75s/it]\n","output_type":"stream"},{"name":"stdout","text":"Dataset Processed!\n > Downloading DVAE files!\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0.00/1.07k [00:00<?, ?iB/s]\n100%|██████████| 1.07k/1.07k [00:00<00:00, 2.68kiB/s]\n\n  1%|          | 1.20M/211M [00:00<00:17, 11.9MiB/s]\u001b[A\n  3%|▎         | 5.92M/211M [00:00<00:06, 32.6MiB/s]\u001b[A\n  5%|▌         | 11.2M/211M [00:00<00:04, 42.0MiB/s]\u001b[A\n  8%|▊         | 16.6M/211M [00:00<00:04, 46.6MiB/s]\u001b[A\n 10%|█         | 22.1M/211M [00:00<00:03, 49.5MiB/s]\u001b[A\n 13%|█▎        | 27.6M/211M [00:00<00:03, 51.3MiB/s]\u001b[A\n 16%|█▌        | 33.0M/211M [00:00<00:03, 52.4MiB/s]\u001b[A\n 18%|█▊        | 38.5M/211M [00:00<00:03, 53.0MiB/s]\u001b[A\n 21%|██        | 43.9M/211M [00:00<00:03, 53.4MiB/s]\u001b[A\n 23%|██▎       | 49.3M/211M [00:01<00:02, 53.7MiB/s]\u001b[A\n 26%|██▌       | 54.8M/211M [00:01<00:02, 53.9MiB/s]\u001b[A\n 29%|██▊       | 60.2M/211M [00:01<00:02, 53.9MiB/s]\u001b[A\n 31%|███       | 65.6M/211M [00:01<00:02, 54.0MiB/s]\u001b[A\n 34%|███▎      | 71.0M/211M [00:01<00:02, 53.4MiB/s]\u001b[A\n 36%|███▋      | 76.5M/211M [00:01<00:02, 54.0MiB/s]\u001b[A\n 39%|███▉      | 82.0M/211M [00:01<00:02, 54.2MiB/s]\u001b[A\n 42%|████▏     | 87.4M/211M [00:01<00:02, 51.9MiB/s]\u001b[A\n 44%|████▍     | 92.6M/211M [00:01<00:02, 51.8MiB/s]\u001b[A\n 47%|████▋     | 97.9M/211M [00:01<00:02, 52.0MiB/s]\u001b[A\n 49%|████▉     | 103M/211M [00:02<00:02, 52.0MiB/s] \u001b[A\n 52%|█████▏    | 109M/211M [00:02<00:01, 53.0MiB/s]\u001b[A\n 54%|█████▍    | 114M/211M [00:02<00:01, 53.8MiB/s]\u001b[A\n 57%|█████▋    | 120M/211M [00:02<00:01, 54.4MiB/s]\u001b[A\n 60%|█████▉    | 125M/211M [00:02<00:01, 54.7MiB/s]\u001b[A\n 62%|██████▏   | 131M/211M [00:02<00:01, 54.8MiB/s]\u001b[A\n 65%|██████▍   | 136M/211M [00:02<00:01, 54.5MiB/s]\u001b[A\n 67%|██████▋   | 142M/211M [00:02<00:01, 54.9MiB/s]\u001b[A\n 70%|███████   | 148M/211M [00:02<00:01, 55.5MiB/s]\u001b[A\n 73%|███████▎  | 153M/211M [00:02<00:01, 55.7MiB/s]\u001b[A\n 75%|███████▌  | 159M/211M [00:03<00:00, 55.3MiB/s]\u001b[A\n 78%|███████▊  | 164M/211M [00:03<00:00, 55.2MiB/s]\u001b[A\n 81%|████████  | 170M/211M [00:03<00:00, 55.3MiB/s]\u001b[A\n 83%|████████▎ | 175M/211M [00:03<00:00, 55.2MiB/s]\u001b[A\n 86%|████████▌ | 181M/211M [00:03<00:00, 54.2MiB/s]\u001b[A\n 89%|████████▊ | 186M/211M [00:03<00:00, 54.4MiB/s]\u001b[A\n 91%|█████████ | 192M/211M [00:03<00:00, 54.5MiB/s]\u001b[A\n 94%|█████████▍| 197M/211M [00:03<00:00, 54.6MiB/s]\u001b[A\n 96%|█████████▋| 203M/211M [00:03<00:00, 54.6MiB/s]\u001b[A\n 99%|█████████▉| 208M/211M [00:03<00:00, 54.9MiB/s]\u001b[A","output_type":"stream"},{"name":"stdout","text":" > Downloading XTTS v2.0 files!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 211M/211M [00:04<00:00, 50.7MiB/s]\n\n100%|██████████| 361k/361k [00:00<00:00, 822kiB/s]\n\n  0%|          | 1.23M/1.87G [00:00<02:33, 12.2MiB/s]\u001b[A\n  0%|          | 5.74M/1.87G [00:00<00:59, 31.5MiB/s]\u001b[A\n  1%|          | 11.0M/1.87G [00:00<00:45, 41.2MiB/s]\u001b[A\n  1%|          | 16.4M/1.87G [00:00<00:40, 46.2MiB/s]\u001b[A\n  1%|          | 21.8M/1.87G [00:00<00:37, 49.0MiB/s]\u001b[A\n  1%|▏         | 27.2M/1.87G [00:00<00:36, 50.7MiB/s]\u001b[A\n  2%|▏         | 32.6M/1.87G [00:00<00:35, 51.8MiB/s]\u001b[A\n  2%|▏         | 37.9M/1.87G [00:00<00:35, 52.1MiB/s]\u001b[A\n  2%|▏         | 43.3M/1.87G [00:00<00:34, 52.8MiB/s]\u001b[A\n  3%|▎         | 48.7M/1.87G [00:01<00:34, 53.2MiB/s]\u001b[A\n  3%|▎         | 54.3M/1.87G [00:01<00:33, 54.0MiB/s]\u001b[A\n  3%|▎         | 59.9M/1.87G [00:01<00:33, 54.5MiB/s]\u001b[A\n  4%|▎         | 65.4M/1.87G [00:01<00:32, 54.8MiB/s]\u001b[A\n  4%|▍         | 71.0M/1.87G [00:01<00:32, 55.0MiB/s]\u001b[A\n  4%|▍         | 76.5M/1.87G [00:01<00:32, 55.1MiB/s]\u001b[A\n  4%|▍         | 82.0M/1.87G [00:01<00:32, 55.2MiB/s]\u001b[A\n  5%|▍         | 87.6M/1.87G [00:01<00:32, 55.2MiB/s]\u001b[A\n  5%|▍         | 93.1M/1.87G [00:01<00:32, 54.9MiB/s]\u001b[A\n  5%|▌         | 98.6M/1.87G [00:01<00:32, 55.0MiB/s]\u001b[A\n  6%|▌         | 104M/1.87G [00:02<00:31, 55.5MiB/s] \u001b[A\n  6%|▌         | 110M/1.87G [00:02<00:31, 55.9MiB/s]\u001b[A\n  6%|▌         | 116M/1.87G [00:02<00:31, 56.0MiB/s]\u001b[A\n  6%|▋         | 121M/1.87G [00:02<00:31, 55.7MiB/s]\u001b[A\n  7%|▋         | 127M/1.87G [00:02<00:31, 55.3MiB/s]\u001b[A\n  7%|▋         | 132M/1.87G [00:02<00:31, 55.2MiB/s]\u001b[A\n  7%|▋         | 138M/1.87G [00:02<00:31, 54.9MiB/s]\u001b[A\n  8%|▊         | 143M/1.87G [00:02<00:31, 55.1MiB/s]\u001b[A\n  8%|▊         | 149M/1.87G [00:02<00:31, 55.2MiB/s]\u001b[A\n  8%|▊         | 155M/1.87G [00:02<00:30, 55.6MiB/s]\u001b[A\n  9%|▊         | 160M/1.87G [00:03<00:30, 55.9MiB/s]\u001b[A\n  9%|▉         | 166M/1.87G [00:03<00:30, 56.0MiB/s]\u001b[A\n  9%|▉         | 172M/1.87G [00:03<00:30, 56.3MiB/s]\u001b[A\n  9%|▉         | 177M/1.87G [00:03<00:30, 56.3MiB/s]\u001b[A\n 10%|▉         | 183M/1.87G [00:03<00:29, 56.4MiB/s]\u001b[A\n 10%|█         | 189M/1.87G [00:03<00:29, 56.6MiB/s]\u001b[A\n 10%|█         | 194M/1.87G [00:03<00:29, 56.8MiB/s]\u001b[A\n 11%|█         | 200M/1.87G [00:03<00:29, 56.8MiB/s]\u001b[A\n 11%|█         | 206M/1.87G [00:03<00:29, 55.7MiB/s]\u001b[A\n 11%|█▏        | 211M/1.87G [00:03<00:29, 55.8MiB/s]\u001b[A\n 12%|█▏        | 217M/1.87G [00:04<00:30, 53.5MiB/s]\u001b[A\n 12%|█▏        | 223M/1.87G [00:04<00:30, 54.4MiB/s]\u001b[A\n 12%|█▏        | 228M/1.87G [00:04<00:29, 55.2MiB/s]\u001b[A\n 13%|█▎        | 234M/1.87G [00:04<00:29, 55.8MiB/s]\u001b[A\n 13%|█▎        | 240M/1.87G [00:04<00:29, 56.0MiB/s]\u001b[A\n 13%|█▎        | 245M/1.87G [00:04<00:28, 56.3MiB/s]\u001b[A\n 13%|█▎        | 251M/1.87G [00:04<00:28, 56.4MiB/s]\u001b[A\n 14%|█▎        | 257M/1.87G [00:04<00:28, 56.4MiB/s]\u001b[A\n 14%|█▍        | 262M/1.87G [00:04<00:28, 55.8MiB/s]\u001b[A\n 14%|█▍        | 268M/1.87G [00:04<00:28, 55.8MiB/s]\u001b[A\n 15%|█▍        | 273M/1.87G [00:05<00:28, 55.9MiB/s]\u001b[A\n 15%|█▍        | 279M/1.87G [00:05<00:28, 56.0MiB/s]\u001b[A\n 15%|█▌        | 285M/1.87G [00:05<00:28, 56.1MiB/s]\u001b[A\n 16%|█▌        | 290M/1.87G [00:05<00:28, 56.3MiB/s]\u001b[A\n 16%|█▌        | 296M/1.87G [00:05<00:27, 56.4MiB/s]\u001b[A\n 16%|█▌        | 302M/1.87G [00:05<00:27, 56.1MiB/s]\u001b[A\n 16%|█▋        | 307M/1.87G [00:05<00:27, 55.8MiB/s]\u001b[A\n 17%|█▋        | 313M/1.87G [00:05<00:28, 54.3MiB/s]\u001b[A\n 17%|█▋        | 318M/1.87G [00:05<00:29, 52.0MiB/s]\u001b[A\n 17%|█▋        | 324M/1.87G [00:05<00:30, 50.5MiB/s]\u001b[A\n 18%|█▊        | 329M/1.87G [00:06<00:31, 49.5MiB/s]\u001b[A\n 18%|█▊        | 334M/1.87G [00:06<00:31, 49.4MiB/s]\u001b[A\n 18%|█▊        | 339M/1.87G [00:06<00:30, 50.0MiB/s]\u001b[A\n 18%|█▊        | 344M/1.87G [00:06<00:29, 51.8MiB/s]\u001b[A\n 19%|█▊        | 350M/1.87G [00:06<00:28, 52.8MiB/s]\u001b[A\n 19%|█▉        | 355M/1.87G [00:06<00:28, 53.8MiB/s]\u001b[A\n 19%|█▉        | 361M/1.87G [00:06<00:27, 54.3MiB/s]\u001b[A\n 20%|█▉        | 367M/1.87G [00:06<00:27, 54.7MiB/s]\u001b[A\n 20%|█▉        | 372M/1.87G [00:06<00:27, 55.1MiB/s]\u001b[A\n 20%|██        | 378M/1.87G [00:06<00:26, 55.5MiB/s]\u001b[A\n 21%|██        | 383M/1.87G [00:07<00:26, 55.8MiB/s]\u001b[A\n 21%|██        | 389M/1.87G [00:07<00:26, 56.0MiB/s]\u001b[A\n 21%|██        | 395M/1.87G [00:07<00:26, 56.2MiB/s]\u001b[A\n 21%|██▏       | 400M/1.87G [00:07<00:26, 56.3MiB/s]\u001b[A\n 22%|██▏       | 406M/1.87G [00:07<00:25, 56.4MiB/s]\u001b[A\n 22%|██▏       | 412M/1.87G [00:07<00:25, 56.4MiB/s]\u001b[A\n 22%|██▏       | 417M/1.87G [00:07<00:25, 56.4MiB/s]\u001b[A\n 23%|██▎       | 423M/1.87G [00:07<00:25, 56.2MiB/s]\u001b[A\n 23%|██▎       | 429M/1.87G [00:07<00:25, 55.9MiB/s]\u001b[A\n 23%|██▎       | 434M/1.87G [00:07<00:25, 56.0MiB/s]\u001b[A\n 24%|██▎       | 440M/1.87G [00:08<00:25, 56.1MiB/s]\u001b[A\n 24%|██▍       | 446M/1.87G [00:08<00:25, 55.6MiB/s]\u001b[A\n 24%|██▍       | 451M/1.87G [00:08<00:25, 55.2MiB/s]\u001b[A\n 24%|██▍       | 457M/1.87G [00:08<00:25, 55.2MiB/s]\u001b[A\n 25%|██▍       | 462M/1.87G [00:08<00:25, 55.2MiB/s]\u001b[A\n 25%|██▌       | 468M/1.87G [00:08<00:25, 55.2MiB/s]\u001b[A\n 25%|██▌       | 473M/1.87G [00:08<00:25, 55.4MiB/s]\u001b[A\n 26%|██▌       | 479M/1.87G [00:08<00:25, 55.1MiB/s]\u001b[A\n 26%|██▌       | 484M/1.87G [00:08<00:25, 55.0MiB/s]\u001b[A\n 26%|██▌       | 490M/1.87G [00:08<00:25, 55.1MiB/s]\u001b[A\n 27%|██▋       | 495M/1.87G [00:09<00:24, 55.2MiB/s]\u001b[A\n 27%|██▋       | 501M/1.87G [00:09<00:24, 55.6MiB/s]\u001b[A\n 27%|██▋       | 507M/1.87G [00:09<00:24, 55.8MiB/s]\u001b[A\n 27%|██▋       | 512M/1.87G [00:09<00:24, 56.1MiB/s]\u001b[A\n 28%|██▊       | 518M/1.87G [00:09<00:24, 56.2MiB/s]\u001b[A\n 28%|██▊       | 524M/1.87G [00:09<00:24, 55.9MiB/s]\u001b[A\n 28%|██▊       | 529M/1.87G [00:09<00:23, 55.8MiB/s]\u001b[A\n 29%|██▊       | 535M/1.87G [00:09<00:23, 55.8MiB/s]\u001b[A\n 29%|██▉       | 540M/1.87G [00:09<00:23, 56.1MiB/s]\u001b[A\n 29%|██▉       | 546M/1.87G [00:09<00:23, 56.2MiB/s]\u001b[A\n 30%|██▉       | 552M/1.87G [00:10<00:23, 56.3MiB/s]\u001b[A\n 30%|██▉       | 557M/1.87G [00:10<00:23, 56.3MiB/s]\u001b[A\n 30%|███       | 563M/1.87G [00:10<00:23, 56.3MiB/s]\u001b[A\n 30%|███       | 569M/1.87G [00:10<00:23, 56.4MiB/s]\u001b[A\n 31%|███       | 574M/1.87G [00:10<00:22, 56.7MiB/s]\u001b[A\n 31%|███       | 580M/1.87G [00:10<00:22, 56.8MiB/s]\u001b[A\n 31%|███▏      | 586M/1.87G [00:10<00:22, 56.9MiB/s]\u001b[A\n 32%|███▏      | 592M/1.87G [00:10<00:22, 56.6MiB/s]\u001b[A\n 32%|███▏      | 597M/1.87G [00:10<00:22, 56.4MiB/s]\u001b[A\n 32%|███▏      | 603M/1.87G [00:11<00:22, 56.1MiB/s]\u001b[A\n 33%|███▎      | 608M/1.87G [00:11<00:22, 55.9MiB/s]\u001b[A\n 33%|███▎      | 614M/1.87G [00:11<00:22, 55.6MiB/s]\u001b[A\n 33%|███▎      | 620M/1.87G [00:11<00:22, 55.5MiB/s]\u001b[A\n 33%|███▎      | 625M/1.87G [00:11<00:22, 55.6MiB/s]\u001b[A\n 34%|███▍      | 631M/1.87G [00:11<00:22, 55.6MiB/s]\u001b[A\n 34%|███▍      | 636M/1.87G [00:11<00:22, 55.6MiB/s]\u001b[A\n 34%|███▍      | 642M/1.87G [00:11<00:22, 55.5MiB/s]\u001b[A\n 35%|███▍      | 647M/1.87G [00:11<00:22, 55.2MiB/s]\u001b[A\n 35%|███▍      | 653M/1.87G [00:11<00:22, 54.7MiB/s]\u001b[A\n 35%|███▌      | 658M/1.87G [00:12<00:22, 54.5MiB/s]\u001b[A\n 36%|███▌      | 664M/1.87G [00:12<00:22, 53.9MiB/s]\u001b[A\n 36%|███▌      | 669M/1.87G [00:12<00:22, 53.6MiB/s]\u001b[A\n 36%|███▌      | 675M/1.87G [00:12<00:22, 53.2MiB/s]\u001b[A\n 36%|███▋      | 680M/1.87G [00:12<00:22, 53.9MiB/s]\u001b[A\n 37%|███▋      | 686M/1.87G [00:12<00:21, 54.1MiB/s]\u001b[A\n 37%|███▋      | 691M/1.87G [00:12<00:21, 54.2MiB/s]\u001b[A\n 37%|███▋      | 697M/1.87G [00:12<00:21, 54.5MiB/s]\u001b[A\n 38%|███▊      | 702M/1.87G [00:12<00:21, 54.8MiB/s]\u001b[A\n 38%|███▊      | 708M/1.87G [00:12<00:21, 54.4MiB/s]\u001b[A\n 38%|███▊      | 713M/1.87G [00:13<00:20, 55.0MiB/s]\u001b[A\n 38%|███▊      | 719M/1.87G [00:13<00:20, 55.6MiB/s]\u001b[A\n 39%|███▉      | 725M/1.87G [00:13<00:20, 56.1MiB/s]\u001b[A\n 39%|███▉      | 730M/1.87G [00:13<00:20, 56.2MiB/s]\u001b[A\n 39%|███▉      | 736M/1.87G [00:13<00:20, 56.5MiB/s]\u001b[A\n 40%|███▉      | 742M/1.87G [00:13<00:19, 56.5MiB/s]\u001b[A\n 40%|████      | 747M/1.87G [00:13<00:19, 56.6MiB/s]\u001b[A\n 40%|████      | 753M/1.87G [00:13<00:19, 56.5MiB/s]\u001b[A\n 41%|████      | 759M/1.87G [00:13<00:20, 53.9MiB/s]\u001b[A\n 41%|████      | 764M/1.87G [00:13<00:21, 52.2MiB/s]\u001b[A\n 41%|████      | 769M/1.87G [00:14<00:21, 51.5MiB/s]\u001b[A\n 41%|████▏     | 775M/1.87G [00:14<00:20, 52.6MiB/s]\u001b[A\n 42%|████▏     | 780M/1.87G [00:14<00:20, 53.6MiB/s]\u001b[A\n 42%|████▏     | 786M/1.87G [00:14<00:20, 54.1MiB/s]\u001b[A\n 42%|████▏     | 792M/1.87G [00:14<00:19, 54.6MiB/s]\u001b[A\n 43%|████▎     | 797M/1.87G [00:14<00:19, 54.9MiB/s]\u001b[A\n 43%|████▎     | 803M/1.87G [00:14<00:19, 55.2MiB/s]\u001b[A\n 43%|████▎     | 808M/1.87G [00:14<00:19, 55.3MiB/s]\u001b[A\n 44%|████▎     | 814M/1.87G [00:14<00:19, 55.1MiB/s]\u001b[A\n 44%|████▍     | 819M/1.87G [00:14<00:19, 55.0MiB/s]\u001b[A\n 44%|████▍     | 825M/1.87G [00:15<00:18, 55.0MiB/s]\u001b[A\n 44%|████▍     | 830M/1.87G [00:15<00:18, 55.4MiB/s]\u001b[A\n 45%|████▍     | 836M/1.87G [00:15<00:18, 55.8MiB/s]\u001b[A\n 45%|████▌     | 842M/1.87G [00:15<00:18, 55.9MiB/s]\u001b[A\n 45%|████▌     | 847M/1.87G [00:15<00:18, 56.2MiB/s]\u001b[A\n 46%|████▌     | 853M/1.87G [00:15<00:18, 56.2MiB/s]\u001b[A\n 46%|████▌     | 859M/1.87G [00:15<00:17, 56.1MiB/s]\u001b[A\n 46%|████▋     | 864M/1.87G [00:15<00:17, 56.0MiB/s]\u001b[A\n 47%|████▋     | 870M/1.87G [00:15<00:17, 56.0MiB/s]\u001b[A\n 47%|████▋     | 876M/1.87G [00:15<00:17, 56.0MiB/s]\u001b[A\n 47%|████▋     | 881M/1.87G [00:16<00:17, 56.2MiB/s]\u001b[A\n 47%|████▋     | 887M/1.87G [00:16<00:24, 39.9MiB/s]\u001b[A\n 48%|████▊     | 892M/1.87G [00:16<00:22, 42.6MiB/s]\u001b[A\n 48%|████▊     | 898M/1.87G [00:16<00:21, 46.0MiB/s]\u001b[A\n 48%|████▊     | 903M/1.87G [00:16<00:19, 48.8MiB/s]\u001b[A\n 49%|████▊     | 909M/1.87G [00:16<00:18, 51.0MiB/s]\u001b[A\n 49%|████▉     | 915M/1.87G [00:16<00:18, 52.6MiB/s]\u001b[A\n 49%|████▉     | 920M/1.87G [00:16<00:17, 53.3MiB/s]\u001b[A\n 50%|████▉     | 926M/1.87G [00:16<00:17, 54.0MiB/s]\u001b[A\n 50%|████▉     | 931M/1.87G [00:17<00:17, 54.8MiB/s]\u001b[A\n 50%|█████     | 937M/1.87G [00:17<00:16, 55.2MiB/s]\u001b[A\n 50%|█████     | 943M/1.87G [00:17<00:16, 55.5MiB/s]\u001b[A\n 51%|█████     | 948M/1.87G [00:17<00:16, 56.0MiB/s]\u001b[A\n 51%|█████     | 954M/1.87G [00:17<00:16, 56.4MiB/s]\u001b[A\n 51%|█████▏    | 960M/1.87G [00:17<00:16, 56.6MiB/s]\u001b[A\n 52%|█████▏    | 965M/1.87G [00:17<00:15, 56.8MiB/s]\u001b[A\n 52%|█████▏    | 971M/1.87G [00:17<00:15, 56.8MiB/s]\u001b[A\n 52%|█████▏    | 977M/1.87G [00:17<00:15, 56.4MiB/s]\u001b[A\n 53%|█████▎    | 982M/1.87G [00:17<00:15, 56.4MiB/s]\u001b[A\n 53%|█████▎    | 988M/1.87G [00:18<00:15, 56.6MiB/s]\u001b[A\n 53%|█████▎    | 994M/1.87G [00:18<00:15, 56.8MiB/s]\u001b[A\n 54%|█████▎    | 1.00G/1.87G [00:18<00:15, 56.6MiB/s]\u001b[A\n 54%|█████▍    | 1.01G/1.87G [00:18<00:15, 56.7MiB/s]\u001b[A\n 54%|█████▍    | 1.01G/1.87G [00:18<00:15, 56.9MiB/s]\u001b[A\n 54%|█████▍    | 1.02G/1.87G [00:18<00:14, 57.0MiB/s]\u001b[A\n 55%|█████▍    | 1.02G/1.87G [00:18<00:14, 57.2MiB/s]\u001b[A\n 55%|█████▌    | 1.03G/1.87G [00:18<00:14, 57.1MiB/s]\u001b[A\n 55%|█████▌    | 1.03G/1.87G [00:18<00:14, 57.0MiB/s]\u001b[A\n 56%|█████▌    | 1.04G/1.87G [00:19<00:14, 57.0MiB/s]\u001b[A\n 56%|█████▌    | 1.05G/1.87G [00:19<00:14, 57.1MiB/s]\u001b[A\n 56%|█████▋    | 1.05G/1.87G [00:19<00:14, 57.0MiB/s]\u001b[A\n 57%|█████▋    | 1.06G/1.87G [00:19<00:14, 57.0MiB/s]\u001b[A\n 57%|█████▋    | 1.06G/1.87G [00:19<00:14, 57.0MiB/s]\u001b[A\n 57%|█████▋    | 1.07G/1.87G [00:19<00:14, 57.1MiB/s]\u001b[A\n 57%|█████▋    | 1.07G/1.87G [00:19<00:13, 57.1MiB/s]\u001b[A\n 58%|█████▊    | 1.08G/1.87G [00:19<00:13, 57.0MiB/s]\u001b[A\n 58%|█████▊    | 1.09G/1.87G [00:19<00:13, 56.7MiB/s]\u001b[A\n 58%|█████▊    | 1.09G/1.87G [00:19<00:13, 56.3MiB/s]\u001b[A\n 59%|█████▊    | 1.10G/1.87G [00:20<00:13, 56.2MiB/s]\u001b[A\n 59%|█████▉    | 1.10G/1.87G [00:20<00:13, 56.3MiB/s]\u001b[A\n 59%|█████▉    | 1.11G/1.87G [00:20<00:13, 56.5MiB/s]\u001b[A\n 60%|█████▉    | 1.11G/1.87G [00:20<00:13, 56.6MiB/s]\u001b[A\n 60%|█████▉    | 1.12G/1.87G [00:20<00:13, 56.7MiB/s]\u001b[A\n 60%|██████    | 1.13G/1.87G [00:20<00:13, 55.8MiB/s]\u001b[A\n 61%|██████    | 1.13G/1.87G [00:20<00:13, 56.0MiB/s]\u001b[A\n 61%|██████    | 1.14G/1.87G [00:20<00:13, 56.1MiB/s]\u001b[A\n 61%|██████    | 1.14G/1.87G [00:20<00:13, 55.6MiB/s]\u001b[A\n 61%|██████▏   | 1.15G/1.87G [00:20<00:12, 55.6MiB/s]\u001b[A\n 62%|██████▏   | 1.15G/1.87G [00:21<00:12, 56.0MiB/s]\u001b[A\n 62%|██████▏   | 1.16G/1.87G [00:21<00:12, 56.3MiB/s]\u001b[A\n 62%|██████▏   | 1.16G/1.87G [00:21<00:12, 56.5MiB/s]\u001b[A\n 63%|██████▎   | 1.17G/1.87G [00:21<00:12, 56.6MiB/s]\u001b[A\n 63%|██████▎   | 1.18G/1.87G [00:21<00:12, 56.6MiB/s]\u001b[A\n 63%|██████▎   | 1.18G/1.87G [00:21<00:12, 54.3MiB/s]\u001b[A\n 64%|██████▎   | 1.19G/1.87G [00:21<00:12, 53.7MiB/s]\u001b[A\n 64%|██████▍   | 1.19G/1.87G [00:21<00:12, 52.9MiB/s]\u001b[A\n 64%|██████▍   | 1.20G/1.87G [00:21<00:12, 51.8MiB/s]\u001b[A\n 64%|██████▍   | 1.20G/1.87G [00:21<00:12, 51.7MiB/s]\u001b[A\n 65%|██████▍   | 1.21G/1.87G [00:22<00:12, 51.8MiB/s]\u001b[A\n 65%|██████▍   | 1.21G/1.87G [00:22<00:12, 51.9MiB/s]\u001b[A\n 65%|██████▌   | 1.22G/1.87G [00:22<00:12, 51.2MiB/s]\u001b[A\n 66%|██████▌   | 1.22G/1.87G [00:22<00:12, 51.5MiB/s]\u001b[A\n 66%|██████▌   | 1.23G/1.87G [00:22<00:12, 52.8MiB/s]\u001b[A\n 66%|██████▌   | 1.23G/1.87G [00:22<00:11, 53.8MiB/s]\u001b[A\n 66%|██████▋   | 1.24G/1.87G [00:22<00:11, 54.5MiB/s]\u001b[A\n 67%|██████▋   | 1.25G/1.87G [00:22<00:11, 55.0MiB/s]\u001b[A\n 67%|██████▋   | 1.25G/1.87G [00:22<00:11, 55.3MiB/s]\u001b[A\n 67%|██████▋   | 1.26G/1.87G [00:22<00:10, 55.5MiB/s]\u001b[A\n 68%|██████▊   | 1.26G/1.87G [00:23<00:10, 55.8MiB/s]\u001b[A\n 68%|██████▊   | 1.27G/1.87G [00:23<00:10, 55.9MiB/s]\u001b[A\n 68%|██████▊   | 1.27G/1.87G [00:23<00:10, 56.0MiB/s]\u001b[A\n 69%|██████▊   | 1.28G/1.87G [00:23<00:10, 55.2MiB/s]\u001b[A\n 69%|██████▉   | 1.29G/1.87G [00:23<00:10, 55.4MiB/s]\u001b[A\n 69%|██████▉   | 1.29G/1.87G [00:23<00:10, 55.6MiB/s]\u001b[A\n 69%|██████▉   | 1.30G/1.87G [00:23<00:10, 55.7MiB/s]\u001b[A\n 70%|██████▉   | 1.30G/1.87G [00:23<00:10, 55.8MiB/s]\u001b[A\n 70%|███████   | 1.31G/1.87G [00:23<00:10, 54.8MiB/s]\u001b[A\n 70%|███████   | 1.31G/1.87G [00:23<00:10, 52.8MiB/s]\u001b[A\n 71%|███████   | 1.32G/1.87G [00:24<00:10, 53.0MiB/s]\u001b[A\n 71%|███████   | 1.32G/1.87G [00:24<00:10, 54.1MiB/s]\u001b[A\n 71%|███████   | 1.33G/1.87G [00:24<00:09, 54.7MiB/s]\u001b[A\n 72%|███████▏  | 1.34G/1.87G [00:24<00:09, 55.4MiB/s]\u001b[A\n 72%|███████▏  | 1.34G/1.87G [00:24<00:09, 55.9MiB/s]\u001b[A\n 72%|███████▏  | 1.35G/1.87G [00:24<00:09, 56.2MiB/s]\u001b[A\n 72%|███████▏  | 1.35G/1.87G [00:24<00:09, 56.2MiB/s]\u001b[A\n 73%|███████▎  | 1.36G/1.87G [00:24<00:09, 56.3MiB/s]\u001b[A\n 73%|███████▎  | 1.36G/1.87G [00:24<00:08, 56.2MiB/s]\u001b[A\n 73%|███████▎  | 1.37G/1.87G [00:24<00:08, 56.2MiB/s]\u001b[A\n 74%|███████▎  | 1.38G/1.87G [00:25<00:08, 56.3MiB/s]\u001b[A\n 74%|███████▍  | 1.38G/1.87G [00:25<00:08, 56.4MiB/s]\u001b[A\n 74%|███████▍  | 1.39G/1.87G [00:25<00:08, 55.8MiB/s]\u001b[A\n 75%|███████▍  | 1.39G/1.87G [00:25<00:08, 55.6MiB/s]\u001b[A\n 75%|███████▍  | 1.40G/1.87G [00:25<00:08, 55.5MiB/s]\u001b[A\n 75%|███████▌  | 1.40G/1.87G [00:25<00:08, 55.5MiB/s]\u001b[A\n 75%|███████▌  | 1.41G/1.87G [00:25<00:08, 55.3MiB/s]\u001b[A\n 76%|███████▌  | 1.41G/1.87G [00:25<00:08, 55.5MiB/s]\u001b[A\n 76%|███████▌  | 1.42G/1.87G [00:25<00:08, 55.5MiB/s]\u001b[A\n 76%|███████▋  | 1.43G/1.87G [00:25<00:07, 55.6MiB/s]\u001b[A\n 77%|███████▋  | 1.43G/1.87G [00:26<00:07, 55.9MiB/s]\u001b[A\n 77%|███████▋  | 1.44G/1.87G [00:26<00:08, 53.8MiB/s]\u001b[A\n 77%|███████▋  | 1.44G/1.87G [00:26<00:07, 54.3MiB/s]\u001b[A\n 78%|███████▊  | 1.45G/1.87G [00:26<00:07, 55.0MiB/s]\u001b[A\n 78%|███████▊  | 1.45G/1.87G [00:26<00:07, 55.1MiB/s]\u001b[A\n 78%|███████▊  | 1.46G/1.87G [00:26<00:07, 55.6MiB/s]\u001b[A\n 78%|███████▊  | 1.46G/1.87G [00:26<00:07, 55.7MiB/s]\u001b[A\n 79%|███████▊  | 1.47G/1.87G [00:26<00:07, 55.7MiB/s]\u001b[A\n 79%|███████▉  | 1.48G/1.87G [00:26<00:07, 55.6MiB/s]\u001b[A\n 79%|███████▉  | 1.48G/1.87G [00:26<00:06, 55.6MiB/s]\u001b[A\n 80%|███████▉  | 1.49G/1.87G [00:27<00:06, 55.7MiB/s]\u001b[A\n 80%|███████▉  | 1.49G/1.87G [00:27<00:06, 55.9MiB/s]\u001b[A\n 80%|████████  | 1.50G/1.87G [00:27<00:06, 56.0MiB/s]\u001b[A\n 81%|████████  | 1.50G/1.87G [00:27<00:06, 56.1MiB/s]\u001b[A\n 81%|████████  | 1.51G/1.87G [00:27<00:06, 56.4MiB/s]\u001b[A\n 81%|████████  | 1.52G/1.87G [00:27<00:06, 55.5MiB/s]\u001b[A\n 81%|████████▏ | 1.52G/1.87G [00:27<00:06, 54.4MiB/s]\u001b[A\n 82%|████████▏ | 1.53G/1.87G [00:27<00:06, 54.8MiB/s]\u001b[A\n 82%|████████▏ | 1.53G/1.87G [00:27<00:06, 55.0MiB/s]\u001b[A\n 82%|████████▏ | 1.54G/1.87G [00:28<00:05, 55.2MiB/s]\u001b[A\n 83%|████████▎ | 1.54G/1.87G [00:28<00:05, 55.5MiB/s]\u001b[A\n 83%|████████▎ | 1.55G/1.87G [00:28<00:05, 55.6MiB/s]\u001b[A\n 83%|████████▎ | 1.55G/1.87G [00:28<00:05, 52.5MiB/s]\u001b[A\n 83%|████████▎ | 1.56G/1.87G [00:28<00:06, 51.2MiB/s]\u001b[A\n 84%|████████▍ | 1.56G/1.87G [00:28<00:06, 50.0MiB/s]\u001b[A\n 84%|████████▍ | 1.57G/1.87G [00:28<00:05, 50.3MiB/s]\u001b[A\n 84%|████████▍ | 1.58G/1.87G [00:28<00:05, 49.8MiB/s]\u001b[A\n 85%|████████▍ | 1.58G/1.87G [00:28<00:05, 51.0MiB/s]\u001b[A\n 85%|████████▍ | 1.59G/1.87G [00:28<00:05, 52.2MiB/s]\u001b[A\n 85%|████████▌ | 1.59G/1.87G [00:29<00:05, 53.0MiB/s]\u001b[A\n 85%|████████▌ | 1.60G/1.87G [00:29<00:05, 53.5MiB/s]\u001b[A\n 86%|████████▌ | 1.60G/1.87G [00:29<00:04, 53.8MiB/s]\u001b[A\n 86%|████████▌ | 1.61G/1.87G [00:29<00:04, 53.8MiB/s]\u001b[A\n 86%|████████▋ | 1.61G/1.87G [00:29<00:04, 54.3MiB/s]\u001b[A\n 87%|████████▋ | 1.62G/1.87G [00:29<00:04, 53.9MiB/s]\u001b[A\n 87%|████████▋ | 1.62G/1.87G [00:29<00:04, 54.5MiB/s]\u001b[A\n 87%|████████▋ | 1.63G/1.87G [00:29<00:04, 55.0MiB/s]\u001b[A\n 88%|████████▊ | 1.64G/1.87G [00:29<00:04, 55.2MiB/s]\u001b[A\n 88%|████████▊ | 1.64G/1.87G [00:29<00:04, 55.3MiB/s]\u001b[A\n 88%|████████▊ | 1.65G/1.87G [00:30<00:04, 55.0MiB/s]\u001b[A\n 88%|████████▊ | 1.65G/1.87G [00:30<00:03, 55.3MiB/s]\u001b[A\n 89%|████████▉ | 1.66G/1.87G [00:30<00:03, 55.6MiB/s]\u001b[A\n 89%|████████▉ | 1.66G/1.87G [00:30<00:03, 55.7MiB/s]\u001b[A\n 89%|████████▉ | 1.67G/1.87G [00:30<00:03, 55.8MiB/s]\u001b[A\n 90%|████████▉ | 1.67G/1.87G [00:30<00:03, 56.1MiB/s]\u001b[A\n 90%|████████▉ | 1.68G/1.87G [00:30<00:03, 56.3MiB/s]\u001b[A\n 90%|█████████ | 1.69G/1.87G [00:30<00:03, 55.2MiB/s]\u001b[A\n 91%|█████████ | 1.69G/1.87G [00:30<00:03, 54.3MiB/s]\u001b[A\n 91%|█████████ | 1.70G/1.87G [00:30<00:03, 53.8MiB/s]\u001b[A\n 91%|█████████ | 1.70G/1.87G [00:31<00:03, 53.3MiB/s]\u001b[A\n 91%|█████████▏| 1.71G/1.87G [00:31<00:03, 53.1MiB/s]\u001b[A\n 92%|█████████▏| 1.71G/1.87G [00:31<00:02, 53.4MiB/s]\u001b[A\n 92%|█████████▏| 1.72G/1.87G [00:31<00:02, 53.6MiB/s]\u001b[A\n 92%|█████████▏| 1.72G/1.87G [00:31<00:02, 54.1MiB/s]\u001b[A\n 93%|█████████▎| 1.73G/1.87G [00:31<00:02, 54.5MiB/s]\u001b[A\n 93%|█████████▎| 1.74G/1.87G [00:31<00:02, 54.9MiB/s]\u001b[A\n 93%|█████████▎| 1.74G/1.87G [00:31<00:02, 55.1MiB/s]\u001b[A\n 93%|█████████▎| 1.75G/1.87G [00:31<00:02, 55.0MiB/s]\u001b[A\n 94%|█████████▍| 1.75G/1.87G [00:31<00:02, 55.2MiB/s]\u001b[A\n 94%|█████████▍| 1.76G/1.87G [00:32<00:01, 55.4MiB/s]\u001b[A\n 94%|█████████▍| 1.76G/1.87G [00:32<00:01, 55.7MiB/s]\u001b[A\n 95%|█████████▍| 1.77G/1.87G [00:32<00:01, 55.8MiB/s]\u001b[A\n 95%|█████████▍| 1.77G/1.87G [00:32<00:01, 55.9MiB/s]\u001b[A\n 95%|█████████▌| 1.78G/1.87G [00:32<00:01, 55.9MiB/s]\u001b[A\n 96%|█████████▌| 1.79G/1.87G [00:32<00:01, 56.2MiB/s]\u001b[A\n 96%|█████████▌| 1.79G/1.87G [00:32<00:01, 56.2MiB/s]\u001b[A\n 96%|█████████▌| 1.80G/1.87G [00:32<00:01, 55.5MiB/s]\u001b[A\n 96%|█████████▋| 1.80G/1.87G [00:32<00:01, 55.6MiB/s]\u001b[A\n 97%|█████████▋| 1.81G/1.87G [00:32<00:01, 55.7MiB/s]\u001b[A\n 97%|█████████▋| 1.81G/1.87G [00:33<00:00, 56.0MiB/s]\u001b[A\n 97%|█████████▋| 1.82G/1.87G [00:33<00:00, 56.1MiB/s]\u001b[A\n 98%|█████████▊| 1.82G/1.87G [00:33<00:00, 56.1MiB/s]\u001b[A\n 98%|█████████▊| 1.83G/1.87G [00:33<00:00, 56.1MiB/s]\u001b[A\n 98%|█████████▊| 1.84G/1.87G [00:33<00:00, 55.6MiB/s]\u001b[A\n 99%|█████████▊| 1.84G/1.87G [00:33<00:00, 55.3MiB/s]\u001b[A\n 99%|█████████▉| 1.85G/1.87G [00:33<00:00, 55.0MiB/s]\u001b[A\n 99%|█████████▉| 1.85G/1.87G [00:33<00:00, 55.1MiB/s]\u001b[A\n 99%|█████████▉| 1.86G/1.87G [00:33<00:00, 54.3MiB/s]\u001b[A\n100%|██████████| 1.87G/1.87G [00:34<00:00, 54.5MiB/s]\u001b[A\nfatal: not a git repository (or any parent up to mount point /kaggle)\nStopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\nfatal: not a git repository (or any parent up to mount point /kaggle)\nStopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n > Training Environment:\n | > Backend: Torch\n | > Mixed precision: False\n | > Precision: float32\n | > Current device: 0\n | > Num. of GPUs: 1\n | > Num. of CPUs: 4\n | > Num. of Torch Threads: 1\n | > Torch seed: 1\n | > Torch CUDNN: True\n | > Torch CUDNN deterministic: False\n | > Torch CUDNN benchmark: False\n | > Torch TF32 MatMul: False\n","output_type":"stream"},{"name":"stdout","text":">> DVAE weights restored from: /kaggle/working/run/training/XTTS_v2.0_original_model_files/dvae.pth\n | > Found 108 files in /kaggle/working/dataset\n","output_type":"stream"},{"name":"stderr","text":"2024-04-18 00:10:07.973188: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-18 00:10:07.973316: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-18 00:10:08.079873: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n > Start Tensorboard: tensorboard --logdir=/kaggle/working/run/training/GPT_XTTS_FT-April-18-2024_12+10AM-0000000\n\n > Model has 518442047 parameters\n\n\u001b[4m\u001b[1m > EPOCH: 0/6\u001b[0m\n --> /kaggle/working/run/training/GPT_XTTS_FT-April-18-2024_12+10AM-0000000\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n\n\u001b[1m > TRAINING (2024-04-18 00:10:15) \u001b[0m\n","output_type":"stream"},{"name":"stdout","text":" > Sampling by language: dict_keys(['en'])\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m   --> TIME: 2024-04-18 00:10:20 -- STEP: 0/54 -- GLOBAL_STEP: 0\u001b[0m\n     | > loss_text_ce: 0.024856537580490112  (0.024856537580490112)\n     | > loss_mel_ce: 4.031044960021973  (4.031044960021973)\n     | > loss: 4.055901527404785  (4.055901527404785)\n     | > grad_norm: 0  (0)\n     | > current_lr: 5e-06 \n     | > step_time: 0.8455  (0.8454740047454834)\n     | > loader_time: 3.94  (3.939969301223755)\n\n\n\u001b[1m   --> TIME: 2024-04-18 00:10:39 -- STEP: 50/54 -- GLOBAL_STEP: 50\u001b[0m\n     | > loss_text_ce: 0.02758578211069107  (0.02325769219547509)\n     | > loss_mel_ce: 3.0128393173217773  (3.256870160102844)\n     | > loss: 3.0404250621795654  (3.280127844810486)\n     | > grad_norm: 0  (0.0)\n     | > current_lr: 5e-06 \n     | > step_time: 0.1325  (0.19674893856048584)\n     | > loader_time: 0.0156  (0.014685564041137697)\n\n","output_type":"stream"},{"name":"stdout","text":" > Filtering invalid eval samples!!\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m > EVALUATION \u001b[0m\n\n","output_type":"stream"},{"name":"stdout","text":" > Total eval samples after filtering: 18\n","output_type":"stream"},{"name":"stderr","text":"\n  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n     | > avg_loader_time: 0.061228781938552856 \u001b[0m(+0)\n     | > avg_loss_text_ce: 0.02089643361978233 \u001b[0m(+0)\n     | > avg_loss_mel_ce: 3.1283965706825256 \u001b[0m(+0)\n     | > avg_loss: 3.149292975664139 \u001b[0m(+0)\n\n > BEST MODEL : /kaggle/working/run/training/GPT_XTTS_FT-April-18-2024_12+10AM-0000000/best_model_54.pth\n\n\u001b[4m\u001b[1m > EPOCH: 1/6\u001b[0m\n --> /kaggle/working/run/training/GPT_XTTS_FT-April-18-2024_12+10AM-0000000\n\n\u001b[1m > TRAINING (2024-04-18 00:11:14) \u001b[0m\n\n\u001b[1m   --> TIME: 2024-04-18 00:11:33 -- STEP: 46/54 -- GLOBAL_STEP: 100\u001b[0m\n     | > loss_text_ce: 0.02476884238421917  (0.02304436343357615)\n     | > loss_mel_ce: 2.805790662765503  (2.9354980406553848)\n     | > loss: 2.830559492111206  (2.9585424143335093)\n     | > grad_norm: 0  (0.0)\n     | > current_lr: 5e-06 \n     | > step_time: 0.1918  (0.20737394042637036)\n     | > loader_time: 0.0069  (0.011675565139107082)\n\n\n\u001b[1m > EVALUATION \u001b[0m\n\n\n  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n     | > avg_loader_time:\u001b[92m 0.05754292011260986 \u001b[0m(-0.003685861825942993)\n     | > avg_loss_text_ce:\u001b[92m 0.020640191854909062 \u001b[0m(-0.0002562417648732662)\n     | > avg_loss_mel_ce:\u001b[92m 3.0465806424617767 \u001b[0m(-0.0818159282207489)\n     | > avg_loss:\u001b[92m 3.067220836877823 \u001b[0m(-0.08207213878631592)\n\n > BEST MODEL : /kaggle/working/run/training/GPT_XTTS_FT-April-18-2024_12+10AM-0000000/best_model_108.pth\n\n\u001b[4m\u001b[1m > EPOCH: 2/6\u001b[0m\n --> /kaggle/working/run/training/GPT_XTTS_FT-April-18-2024_12+10AM-0000000\n\n\u001b[1m > TRAINING (2024-04-18 00:12:08) \u001b[0m\n\n\u001b[1m   --> TIME: 2024-04-18 00:12:25 -- STEP: 42/54 -- GLOBAL_STEP: 150\u001b[0m\n     | > loss_text_ce: 0.027483312413096428  (0.022902643529786947)\n     | > loss_mel_ce: 2.60579776763916  (2.844279397101629)\n     | > loss: 2.6332809925079346  (2.867182033402579)\n     | > grad_norm: 0  (0.0)\n     | > current_lr: 5e-06 \n     | > step_time: 0.2365  (0.21300934609912692)\n     | > loader_time: 0.0076  (0.009513718741280692)\n\n\n\u001b[1m > EVALUATION \u001b[0m\n\n\n  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n     | > avg_loader_time:\u001b[92m 0.05516183376312256 \u001b[0m(-0.0023810863494873047)\n     | > avg_loss_text_ce:\u001b[92m 0.020517053897492588 \u001b[0m(-0.00012313795741647482)\n     | > avg_loss_mel_ce:\u001b[92m 3.003861367702484 \u001b[0m(-0.0427192747592926)\n     | > avg_loss:\u001b[92m 3.0243784189224243 \u001b[0m(-0.04284241795539856)\n\n > BEST MODEL : /kaggle/working/run/training/GPT_XTTS_FT-April-18-2024_12+10AM-0000000/best_model_162.pth\n\n\u001b[4m\u001b[1m > EPOCH: 3/6\u001b[0m\n --> /kaggle/working/run/training/GPT_XTTS_FT-April-18-2024_12+10AM-0000000\n\n\u001b[1m > TRAINING (2024-04-18 00:13:02) \u001b[0m\n\n\u001b[1m   --> TIME: 2024-04-18 00:13:18 -- STEP: 38/54 -- GLOBAL_STEP: 200\u001b[0m\n     | > loss_text_ce: 0.02536611072719097  (0.022393185939443738)\n     | > loss_mel_ce: 2.6723287105560303  (2.7174132434945357)\n     | > loss: 2.697694778442383  (2.7398064261988586)\n     | > grad_norm: 0  (0.0)\n     | > current_lr: 5e-06 \n     | > step_time: 0.1916  (0.21028719450298108)\n     | > loader_time: 0.0076  (0.00920526604903372)\n\n\n\u001b[1m > EVALUATION \u001b[0m\n\n\n  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n     | > avg_loader_time:\u001b[91m 0.05746689438819885 \u001b[0m(+0.002305060625076294)\n     | > avg_loss_text_ce:\u001b[92m 0.02050060231704265 \u001b[0m(-1.6451580449938774e-05)\n     | > avg_loss_mel_ce:\u001b[92m 2.9753231704235077 \u001b[0m(-0.02853819727897644)\n     | > avg_loss:\u001b[92m 2.99582377076149 \u001b[0m(-0.028554648160934448)\n\n > BEST MODEL : /kaggle/working/run/training/GPT_XTTS_FT-April-18-2024_12+10AM-0000000/best_model_216.pth\n\n\u001b[4m\u001b[1m > EPOCH: 4/6\u001b[0m\n --> /kaggle/working/run/training/GPT_XTTS_FT-April-18-2024_12+10AM-0000000\n\n\u001b[1m > TRAINING (2024-04-18 00:13:59) \u001b[0m\n\n\u001b[1m   --> TIME: 2024-04-18 00:14:14 -- STEP: 34/54 -- GLOBAL_STEP: 250\u001b[0m\n     | > loss_text_ce: 0.020914150401949883  (0.021939752459087792)\n     | > loss_mel_ce: 2.5102930068969727  (2.6752976810230926)\n     | > loss: 2.5312070846557617  (2.6972374214845547)\n     | > grad_norm: 0  (0.0)\n     | > current_lr: 5e-06 \n     | > step_time: 0.1171  (0.210501186987933)\n     | > loader_time: 0.0155  (0.009010364027584301)\n\n\n\u001b[1m > EVALUATION \u001b[0m\n\n\n  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n     | > avg_loader_time:\u001b[92m 0.05525967478752136 \u001b[0m(-0.0022072196006774902)\n     | > avg_loss_text_ce:\u001b[92m 0.020472683245316148 \u001b[0m(-2.7919071726500988e-05)\n     | > avg_loss_mel_ce:\u001b[91m 2.989010065793991 \u001b[0m(+0.013686895370483398)\n     | > avg_loss:\u001b[91m 3.0094827711582184 \u001b[0m(+0.013659000396728516)\n\n\n\u001b[4m\u001b[1m > EPOCH: 5/6\u001b[0m\n --> /kaggle/working/run/training/GPT_XTTS_FT-April-18-2024_12+10AM-0000000\n\n\u001b[1m > TRAINING (2024-04-18 00:14:23) \u001b[0m\n\n\u001b[1m   --> TIME: 2024-04-18 00:14:36 -- STEP: 30/54 -- GLOBAL_STEP: 300\u001b[0m\n     | > loss_text_ce: 0.024204809218645096  (0.023284710322817165)\n     | > loss_mel_ce: 2.7359044551849365  (2.5385255495707195)\n     | > loss: 2.7601091861724854  (2.5618102788925166)\n     | > grad_norm: 0  (0.0)\n     | > current_lr: 5e-06 \n     | > step_time: 0.209  (0.21125345230102538)\n     | > loader_time: 0.0092  (0.008898774782816568)\n\n\n\u001b[1m > EVALUATION \u001b[0m\n\n\n  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n     | > avg_loader_time:\u001b[91m 0.056093305349349976 \u001b[0m(+0.0008336305618286133)\n     | > avg_loss_text_ce:\u001b[92m 0.020422062487341464 \u001b[0m(-5.062075797468424e-05)\n     | > avg_loss_mel_ce:\u001b[92m 2.9773911833763123 \u001b[0m(-0.011618882417678833)\n     | > avg_loss:\u001b[92m 2.997813254594803 \u001b[0m(-0.011669516563415527)\n\n","output_type":"stream"},{"name":"stdout","text":"Model training done!\n","output_type":"stream"}]},{"cell_type":"code","source":"!mv /kaggle/working/run/training/GPT_XTTS_FT-April-18-2024_12+10AM-0000000/best_model_216.pth /kaggle/working/run/training/GPT_XTTS_FT-April-18-2024_12+10AM-0000000/model.pth","metadata":{"execution":{"iopub.status.busy":"2024-04-18T00:17:29.763581Z","iopub.execute_input":"2024-04-18T00:17:29.764523Z","iopub.status.idle":"2024-04-18T00:17:29.995345Z","shell.execute_reply.started":"2024-04-18T00:17:29.764481Z","shell.execute_reply":"2024-04-18T00:17:29.993983Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"out_wav_file =\"/kaggle/working/out.wav\"","metadata":{"execution":{"iopub.status.busy":"2024-04-18T00:14:47.736719Z","iopub.execute_input":"2024-04-18T00:14:47.737056Z","iopub.status.idle":"2024-04-18T00:14:47.741922Z","shell.execute_reply.started":"2024-04-18T00:14:47.737027Z","shell.execute_reply":"2024-04-18T00:14:47.740755Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"\n\n!tts --model_path /kaggle/working/run/training/GPT_XTTS_FT-April-18-2024_12+10AM-0000000/ \\\n     --config_path /kaggle/working/run/training/XTTS_v2.0_original_model_files/config.json \\\n     --text \"I am the very model of a modern Major-General,\\\n             I've information vegetable, animal, and mineral, \\\n             I know the kings of England, and I quote the fights historical \\\n             From Marathon to Waterloo, in order categorical; \\\n             I'm very well acquainted, too, with matters mathematical.\" \\\n     --speaker_wav /kaggle/working/dataset/wavs/clip113_00000001.wav \\\n     --language_idx en \\\n     --use_cuda true \\\n     --out_path $out_wav_file\n","metadata":{"execution":{"iopub.status.busy":"2024-04-18T00:14:47.743746Z","iopub.execute_input":"2024-04-18T00:14:47.744319Z","iopub.status.idle":"2024-04-18T00:15:27.036390Z","shell.execute_reply.started":"2024-04-18T00:14:47.744287Z","shell.execute_reply":"2024-04-18T00:15:27.035401Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":" > Using model: xtts\nTraceback (most recent call last):\n  File \"/opt/conda/bin/tts\", line 8, in <module>\n    sys.exit(main())\n  File \"/opt/conda/lib/python3.10/site-packages/TTS/bin/synthesize.py\", line 423, in main\n    synthesizer = Synthesizer(\n  File \"/opt/conda/lib/python3.10/site-packages/TTS/utils/synthesizer.py\", line 93, in __init__\n    self._load_tts(tts_checkpoint, tts_config_path, use_cuda)\n  File \"/opt/conda/lib/python3.10/site-packages/TTS/utils/synthesizer.py\", line 192, in _load_tts\n    self.tts_model.load_checkpoint(self.tts_config, tts_checkpoint, eval=True)\n  File \"/opt/conda/lib/python3.10/site-packages/TTS/tts/models/xtts.py\", line 771, in load_checkpoint\n    checkpoint = self.get_compatible_checkpoint_state_dict(model_path)\n  File \"/opt/conda/lib/python3.10/site-packages/TTS/tts/models/xtts.py\", line 714, in get_compatible_checkpoint_state_dict\n    checkpoint = load_fsspec(model_path, map_location=torch.device(\"cpu\"))[\"model\"]\n  File \"/opt/conda/lib/python3.10/site-packages/TTS/utils/io.py\", line 46, in load_fsspec\n    with fsspec.open(\n  File \"/opt/conda/lib/python3.10/site-packages/fsspec/core.py\", line 103, in __enter__\n    f = self.fs.open(self.path, mode=mode)\n  File \"/opt/conda/lib/python3.10/site-packages/fsspec/implementations/cached.py\", line 436, in <lambda>\n    return lambda *args, **kw: getattr(type(self), item).__get__(self)(\n  File \"/opt/conda/lib/python3.10/site-packages/fsspec/spec.py\", line 1293, in open\n    f = self._open(\n  File \"/opt/conda/lib/python3.10/site-packages/fsspec/implementations/cached.py\", line 436, in <lambda>\n    return lambda *args, **kw: getattr(type(self), item).__get__(self)(\n  File \"/opt/conda/lib/python3.10/site-packages/fsspec/implementations/cached.py\", line 682, in _open\n    fn = self._make_local_details(path)\n  File \"/opt/conda/lib/python3.10/site-packages/fsspec/implementations/cached.py\", line 436, in <lambda>\n    return lambda *args, **kw: getattr(type(self), item).__get__(self)(\n  File \"/opt/conda/lib/python3.10/site-packages/fsspec/implementations/cached.py\", line 599, in _make_local_details\n    \"uid\": self.fs.ukey(path),\n  File \"/opt/conda/lib/python3.10/site-packages/fsspec/spec.py\", line 1332, in ukey\n    return sha256(str(self.info(path)).encode()).hexdigest()\n  File \"/opt/conda/lib/python3.10/site-packages/fsspec/implementations/local.py\", line 87, in info\n    out = os.stat(path, follow_symlinks=False)\nFileNotFoundError: [Errno 2] No such file or directory: '/kaggle/working/run/training/GPT_XTTS_FT-March-20-2024_01+20PM-0000000/model.pth'\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"out2_wav_file =\"/kaggle/working/out2.wav\"","metadata":{"execution":{"iopub.status.busy":"2024-04-18T00:18:10.993793Z","iopub.execute_input":"2024-04-18T00:18:10.994905Z","iopub.status.idle":"2024-04-18T00:18:11.002985Z","shell.execute_reply.started":"2024-04-18T00:18:10.994856Z","shell.execute_reply":"2024-04-18T00:18:11.002065Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"\n\n!tts --model_path /kaggle/working/run/training/GPT_XTTS_FT-April-18-2024_12+10AM-0000000/ \\\n     --config_path /kaggle/working/run/training/XTTS_v2.0_original_model_files/config.json \\\n     --text \"Этот код и модель являются воплощением профессионализма и творчества ,\\\n             Я выражаю свою глубокую благодарность за эту потрясающую работу, \\\n             Мне было удивительно видеть, как идеи превращаются в реальность, и это стимулирует меня к дальнейшему развитию и совершенствованию \\\n             Я также хотел бы отметить замечательное сотрудничество и поддержку, оказанную мне во время выполнения этого проекта; \\\n             Спасибо вам за ваше профессиональное отношение и вдохновение!.\" \\\n     --speaker_wav /kaggle/input/recording/audio_2024-03-16_22-44-54.wav \\\n     --language_idx ru \\\n     --use_cuda true \\\n     --out_path $out2_wav_file","metadata":{"execution":{"iopub.status.busy":"2024-04-18T00:18:22.686083Z","iopub.execute_input":"2024-04-18T00:18:22.686789Z","iopub.status.idle":"2024-04-18T00:19:28.144213Z","shell.execute_reply.started":"2024-04-18T00:18:22.686759Z","shell.execute_reply":"2024-04-18T00:19:28.142995Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":" > Using model: xtts\n > Text: Этот код и модель являются воплощением профессионализма и творчества ,              Я выражаю свою глубокую благодарность за эту потрясающую работу,               Мне было удивительно видеть, как идеи превращаются в реальность, и это стимулирует меня к дальнейшему развитию и совершенствованию               Я также хотел бы отметить замечательное сотрудничество и поддержку, оказанную мне во время выполнения этого проекта;               Спасибо вам за ваше профессиональное отношение и вдохновение!.\n > Text splitted to sentences.\n['Этот код и модель являются воплощением профессионализма и творчества ,              Я выражаю свою глубокую благодарность за эту потрясающую работу,               Мне было удивительно видеть, как идеи превращаются в реальность, и это стимулирует меня к дальнейшему развитию и совершенствованию               Я также хотел бы отметить замечательное сотрудничество и поддержку, оказанную мне во время выполнения этого проекта;               Спасибо вам за ваше профессиональное отношение и вдохновение!', '.']\n[!] Warning: The text length exceeds the character limit of 182 for language 'ru', this might cause truncated audio.\n > Processing time: 11.901952981948853\n > Real-time factor: 0.5733092958143399\n > Saving output to /kaggle/working/out2.wav\n\u001b[0m","output_type":"stream"}]}]}